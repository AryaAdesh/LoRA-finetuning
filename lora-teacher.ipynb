{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7c6ac711-c486-487f-b123-edf2a42bc9cf",
    "_uuid": "852aeb54-454d-4255-a61c-cbb4dfad6314",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Notebook 4: LoRA Teacher Model Training (roberta-large on AG News)\n",
    "Purpose:\n",
    "1. Load pre-processed original and augmented AG News data.\n",
    "2. Combine original and augmented data.\n",
    "3. Split the combined data into Train and Validation sets (rule compliant).\n",
    "4. Load roberta-large model and apply LoRA configuration.\n",
    "5. Configure Trainer for LoRA fine-tuning.\n",
    "6. Fine-tune the LoRA roberta-large model on the new train split, validating on the new validation split.\n",
    "7. Save the fine-tuned LoRA adapter to be used as a teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "ed0ca9a2-abdd-4ce2-a651-3984cc0ceda5",
    "_uuid": "2cf55921-37ae-40cf-b89e-fcb1c95af2bf",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-17T17:58:42.605824Z",
     "iopub.status.busy": "2025-04-17T17:58:42.605282Z",
     "iopub.status.idle": "2025-04-17T17:58:42.754848Z",
     "shell.execute_reply": "2025-04-17T17:58:42.754124Z",
     "shell.execute_reply.started": "2025-04-17T17:58:42.605801Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up environment...\n",
      "INFO: Hugging Face datasets cache directory set to: /kaggle/working/hf_datasets_cache\n"
     ]
    }
   ],
   "source": [
    "# --- Essential Setup ---\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "import random\n",
    "import shutil\n",
    "import gc\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from datasets import load_dataset, Dataset, ClassLabel, load_from_disk, concatenate_datasets, Features, Value\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification, \n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainerCallback,\n",
    "    SchedulerType,\n",
    "    TrainerState,\n",
    "    TrainerControl,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model,TaskType \n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "print(\"Setting up environment...\")\n",
    "!rm -rf /kaggle/working/*\n",
    "\n",
    "# --- Cache Directory Setup ---\n",
    "cache_dir = \"/kaggle/working/hf_datasets_cache\"\n",
    "os.environ['HF_DATASETS_CACHE'] = cache_dir\n",
    "os.environ['DATASETS_CACHE'] = cache_dir\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "print(f\"INFO: Hugging Face datasets cache directory set to: {os.environ.get('HF_DATASETS_CACHE')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "c69d59d9-4a98-470c-9218-4a34fa7db826",
    "_uuid": "ac43f667-c49d-4957-a7a6-773d11726706",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-17T17:57:19.254987Z",
     "iopub.status.busy": "2025-04-17T17:57:19.254392Z",
     "iopub.status.idle": "2025-04-17T17:57:19.258982Z",
     "shell.execute_reply": "2025-04-17T17:57:19.258209Z",
     "shell.execute_reply.started": "2025-04-17T17:57:19.254964Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "teacher_base_model_name = 'roberta-large' \n",
    "dataset_name = 'ag_news'\n",
    "\n",
    "# Paths to pre-processed data\n",
    "cleaned_original_load_path = \"/kaggle/input/cleanedorig\"\n",
    "tokenized_augmented_load_path = \"/kaggle/input/cleanedaugmenteddata\"\n",
    "\n",
    "# Output paths for the LoRA TEACHER model adapter\n",
    "teacher_output_dir = \"/kaggle/working/lora_teacher_training_output\" # Training checkpoints/logs\n",
    "teacher_adapter_save_path = \"/kaggle/working/roberta_large_lora_teacher_adapter\" # Final saved adapter\n",
    "\n",
    "# --- LoRA settings FOR TEACHER ---\n",
    "TEACHER_LORA_R = 16         \n",
    "TEACHER_LORA_ALPHA = 32        \n",
    "TEACHER_LORA_DROPOUT = 0.1    \n",
    "TEACHER_LORA_TARGET_MODULES = ['query', 'value', 'key']\n",
    "# Tokenizer settings\n",
    "TOKENIZER_MAX_LENGTH = 512\n",
    "\n",
    "# Validation split size\n",
    "VALIDATION_SET_SIZE = 0.1 # 10% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "85c31026-fefd-4420-b040-d211c8cdefd9",
    "_uuid": "0ef40917-f0ac-4c84-888e-3fc2ebe33b89",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-17T17:57:20.555617Z",
     "iopub.status.busy": "2025-04-17T17:57:20.554984Z",
     "iopub.status.idle": "2025-04-17T17:57:20.560026Z",
     "shell.execute_reply": "2025-04-17T17:57:20.559430Z",
     "shell.execute_reply.started": "2025-04-17T17:57:20.555592Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. Using device: cuda\n",
      "GPU Name: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "# --- GPU Check ---\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU is available. Using device: {device}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"WARNING: GPU not available, using CPU. Training will be slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "aff03751-7535-4f72-961a-ee130eafd14a",
    "_uuid": "da36ca41-616f-4780-91e9-5f4603c94d2c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-17T17:57:21.615281Z",
     "iopub.status.busy": "2025-04-17T17:57:21.614747Z",
     "iopub.status.idle": "2025-04-17T17:57:21.845798Z",
     "shell.execute_reply": "2025-04-17T17:57:21.845022Z",
     "shell.execute_reply.started": "2025-04-17T17:57:21.615257Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for: roberta-large\n"
     ]
    }
   ],
   "source": [
    "# --- Load Tokenizer ---\n",
    "print(f\"Loading tokenizer for: {teacher_base_model_name}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(teacher_base_model_name)\n",
    "except Exception as e: print(f\"ERROR: Failed to load tokenizer: {e}\"); raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "c3efb727-6375-4cae-894f-f663240a8918",
    "_uuid": "eecffcae-6dd1-4136-9288-94415f8f3e72",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-17T17:57:26.195331Z",
     "iopub.status.busy": "2025-04-17T17:57:26.194631Z",
     "iopub.status.idle": "2025-04-17T17:57:26.199289Z",
     "shell.execute_reply": "2025-04-17T17:57:26.198556Z",
     "shell.execute_reply.started": "2025-04-17T17:57:26.195306Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Label Info ---\n",
    "num_labels = 4\n",
    "id2label = {0: 'World', 1: 'Sports', 2: 'Business', 3: 'Sci/Tech'}\n",
    "label2id = {'World': 0, 'Sports': 1, 'Business': 2, 'Sci/Tech': 3}\n",
    "class_names = list(id2label.values())\n",
    "labels_feature_definition = ClassLabel(num_classes=num_labels, names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "bd723650-64cb-4055-9e3a-b0b02f994ab2",
    "_uuid": "5effaa81-fb4d-453f-b6a1-ecfe1e280a6f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-17T17:57:27.455461Z",
     "iopub.status.busy": "2025-04-17T17:57:27.454896Z",
     "iopub.status.idle": "2025-04-17T17:57:28.844454Z",
     "shell.execute_reply": "2025-04-17T17:57:28.843738Z",
     "shell.execute_reply.started": "2025-04-17T17:57:27.455434Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing Original + Augmented AG News data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5946b03971f347e3915737d9f5172131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/114832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Original dataset reloaded (114832 examples).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a95db89e1a40f4b75c5a98c705f34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/114832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c50d302e6ec41dbb31290f68e848089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/114832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Augmented dataset reloaded (114832 examples).\n",
      "INFO: Combining RELOADED original and RELOADED augmented datasets...\n",
      "Combined dataset created (Orig+Aug) with 229664 examples.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading and preparing Original + Augmented AG News data...\")\n",
    "# --- Load CLEANED ORIGINAL data ---\n",
    "original_ds_reloaded = None\n",
    "if os.path.exists(cleaned_original_load_path):\n",
    "    try:\n",
    "        # ... (Insert save/reload logic for original_ds_reloaded here, including feature casting) ...\n",
    "        loaded_cleaned_original_ds_input = load_from_disk(cleaned_original_load_path)\n",
    "        temp_orig_save_path = \"/kaggle/working/original_dataset_temp\"; loaded_cleaned_original_ds_input.save_to_disk(temp_orig_save_path)\n",
    "        original_features_ref = loaded_cleaned_original_ds_input.features; del loaded_cleaned_original_ds_input; gc.collect(); torch.cuda.empty_cache()\n",
    "        original_ds_reloaded = load_from_disk(temp_orig_save_path)\n",
    "        if original_ds_reloaded.features != original_features_ref: original_ds_reloaded = original_ds_reloaded.cast(original_features_ref)\n",
    "        original_labels_feature = original_ds_reloaded.features['labels'] # Get definitive label feature\n",
    "        if original_labels_feature.num_classes != num_labels or original_labels_feature.names != class_names: # Recast if needed\n",
    "             original_ds_reloaded = original_ds_reloaded.cast_column('labels', labels_feature_definition); original_labels_feature = original_ds_reloaded.features['labels']\n",
    "        print(f\"INFO: Original dataset reloaded ({len(original_ds_reloaded)} examples).\")\n",
    "        try: shutil.rmtree(temp_orig_save_path)\n",
    "        except Exception as e_rm: print(f\"WARNING: Could not remove {temp_orig_save_path}: {e_rm}\")\n",
    "    except Exception as e: print(f\"ERROR loading/processing original: {e}\"); raise e\n",
    "else: raise FileNotFoundError(f\"Path not found: {cleaned_original_load_path}\")\n",
    "\n",
    "# --- Load TOKENIZED AUGMENTED data ---\n",
    "augmented_ds_reloaded = None\n",
    "if os.path.exists(tokenized_augmented_load_path):\n",
    "     try:\n",
    "         loaded_tokenized_augmented_ds_input = load_from_disk(tokenized_augmented_load_path)\n",
    "         temp_aug_save_path = \"/kaggle/working/augmented_dataset_temp\"; loaded_tokenized_augmented_ds_input.save_to_disk(temp_aug_save_path)\n",
    "         augmented_features_ref = loaded_tokenized_augmented_ds_input.features; del loaded_tokenized_augmented_ds_input; gc.collect(); torch.cuda.empty_cache()\n",
    "         augmented_ds_reloaded = load_from_disk(temp_aug_save_path)\n",
    "         if augmented_ds_reloaded.features != augmented_features_ref: augmented_ds_reloaded = augmented_ds_reloaded.cast(augmented_features_ref)\n",
    "         augmented_ds_reloaded = augmented_ds_reloaded.cast_column('labels', original_labels_feature) # Cast labels using feature from original\n",
    "         print(f\"INFO: Augmented dataset reloaded ({len(augmented_ds_reloaded)} examples).\")\n",
    "         try: shutil.rmtree(temp_aug_save_path)\n",
    "         except Exception as e_rm: print(f\"WARNING: Could not remove {temp_aug_save_path}: {e_rm}\")\n",
    "     except Exception as e: print(f\"ERROR loading/processing augmented: {e}\"); raise e\n",
    "else: raise FileNotFoundError(f\"Path not found: {tokenized_augmented_load_path}\")\n",
    "\n",
    "# --- Combine Original + Augmented Data ---\n",
    "print(\"INFO: Combining RELOADED original and RELOADED augmented datasets...\")\n",
    "required_columns = ['input_ids', 'attention_mask', 'labels']\n",
    "try:\n",
    "    train_dataset_for_concat = original_ds_reloaded.select_columns(required_columns)\n",
    "    tokenized_augmented_dataset_for_concat = augmented_ds_reloaded.select_columns(required_columns)\n",
    "except Exception as e: print(f\"ERROR during column selection/prep for concat: {e}\"); raise e\n",
    "combined_dataset_all = concatenate_datasets([train_dataset_for_concat, tokenized_augmented_dataset_for_concat])\n",
    "print(f\"Combined dataset created (Orig+Aug) with {len(combined_dataset_all)} examples.\")\n",
    "del train_dataset_for_concat, tokenized_augmented_dataset_for_concat, original_ds_reloaded, augmented_ds_reloaded\n",
    "gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "61df992f-a469-4093-9081-1682a12d322a",
    "_uuid": "5befeb51-d153-4f73-86e9-15474ff40cef",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-17T17:57:29.124891Z",
     "iopub.status.busy": "2025-04-17T17:57:29.124532Z",
     "iopub.status.idle": "2025-04-17T17:57:29.613165Z",
     "shell.execute_reply": "2025-04-17T17:57:29.612388Z",
     "shell.execute_reply.started": "2025-04-17T17:57:29.124860Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Splitting combined data into Train/Validation (90%/10%)...\n",
      "Teacher Training set size: 206697\n",
      "Teacher Validation set size: 22967\n"
     ]
    }
   ],
   "source": [
    "# --- Create Train/Validation Split from Combined Data ---\n",
    "print(f\"INFO: Splitting combined data into Train/Validation ({1.0-VALIDATION_SET_SIZE:.0%}/{VALIDATION_SET_SIZE:.0%})...\")\n",
    "combined_dataset_shuffled = combined_dataset_all.shuffle(seed=42)\n",
    "split_datasets = combined_dataset_shuffled.train_test_split(test_size=VALIDATION_SET_SIZE, seed=42, shuffle=False) # Already shuffled\n",
    "\n",
    "teacher_train_dataset = split_datasets['train']\n",
    "teacher_eval_dataset = split_datasets['test']\n",
    "\n",
    "print(f\"Teacher Training set size: {len(teacher_train_dataset)}\")\n",
    "print(f\"Teacher Validation set size: {len(teacher_eval_dataset)}\")\n",
    "del combined_dataset_all, combined_dataset_shuffled\n",
    "gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "15d30820-a2ef-4970-9fb6-f2e10d6ddb59",
    "_uuid": "ae764fc7-616f-4531-b731-fd1cb184928b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-17T17:58:49.495475Z",
     "iopub.status.busy": "2025-04-17T17:58:49.494804Z",
     "iopub.status.idle": "2025-04-17T17:58:49.938150Z",
     "shell.execute_reply": "2025-04-17T17:58:49.937644Z",
     "shell.execute_reply.started": "2025-04-17T17:58:49.495450Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading teacher base model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher base model loaded.\n",
      "Applying LoRA config to Teacher: r=16, alpha=32, dropout=0.1, targets=['query', 'value', 'key']\n",
      "LoRA applied to teacher model.\n",
      "trainable params: 3,412,996 || all params: 358,776,840 || trainable%: 0.9513\n"
     ]
    }
   ],
   "source": [
    "# --- Prepare LoRA Teacher Model ---\n",
    "print(f\"Loading teacher base model: {teacher_base_model_name}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_base_model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "print(\"Teacher base model loaded.\")\n",
    "\n",
    "print(f\"Applying LoRA config to Teacher: r={TEACHER_LORA_R}, alpha={TEACHER_LORA_ALPHA}, dropout={TEACHER_LORA_DROPOUT}, targets={TEACHER_LORA_TARGET_MODULES}\")\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=TEACHER_LORA_R,\n",
    "    lora_alpha=TEACHER_LORA_ALPHA,\n",
    "    target_modules=TEACHER_LORA_TARGET_MODULES,\n",
    "    lora_dropout=TEACHER_LORA_DROPOUT\n",
    ")\n",
    "# Apply LoRA to the roberta-large model\n",
    "model = get_peft_model(model, lora_config) # model variable now holds the PEFT model\n",
    "\n",
    "print(\"LoRA applied to teacher model.\")\n",
    "model.print_trainable_parameters() # Print LoRA parameter count for teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "bf817c15-f574-4a77-896c-b29a59ba1c6e",
    "_uuid": "6d0e861b-be78-49fc-a898-55f39f6bcdbf",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-17T17:59:08.800547Z",
     "iopub.status.busy": "2025-04-17T17:59:08.800226Z",
     "iopub.status.idle": "2025-04-17T17:59:08.806463Z",
     "shell.execute_reply": "2025-04-17T17:59:08.805777Z",
     "shell.execute_reply.started": "2025-04-17T17:59:08.800519Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Define Metrics & Collator ---\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "class MetricsCollectorCallback(TrainerCallback):\n",
    "    def __init__(self): self.logs = []\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None: self.logs.append((state.global_step, logs))\n",
    "        return control\n",
    "metrics_collector = MetricsCollectorCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "3f8030a8-6363-4a1c-a2c4-f4cf6f7d3c61",
    "_uuid": "918d4921-6f95-4e2e-923e-4f05aaab5774",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-17T18:00:01.235218Z",
     "iopub.status.busy": "2025-04-17T18:00:01.234970Z",
     "iopub.status.idle": "2025-04-17T18:00:01.266531Z",
     "shell.execute_reply": "2025-04-17T18:00:01.265782Z",
     "shell.execute_reply.started": "2025-04-17T18:00:01.235201Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining Training Arguments for LoRA Teacher Model...\n"
     ]
    }
   ],
   "source": [
    "# --- Define Training Arguments for LoRA Teacher Model ---\n",
    "print(\"Defining Training Arguments for LoRA Teacher Model...\")\n",
    "# Adjust parameters for LoRA fine-tuning roberta-large\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=teacher_output_dir,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2, # Keep best 2 checkpoints of the adapter\n",
    "\n",
    "    learning_rate=5e-5,       \n",
    "    lr_scheduler_type=SchedulerType.LINEAR, # Or cosine\n",
    "    warmup_ratio=0.06,\n",
    "\n",
    "    # Batch size can likely be larger than full fine-tuning large model\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=4, # effective batch size (16*4=64)\n",
    "\n",
    "    num_train_epochs=1,        # Start with 1 epoch for LoRA teacher\n",
    "\n",
    "    weight_decay=0.1,         \n",
    "    label_smoothing_factor=0.1,\n",
    "\n",
    "    load_best_model_at_end=True, \n",
    "    metric_for_best_model=\"accuracy\", \n",
    "    greater_is_better=True,\n",
    "\n",
    "    fp16=torch.cuda.is_available(),\n",
    "\n",
    "    dataloader_num_workers=2,\n",
    "    report_to=[],\n",
    "    logging_first_step=True,\n",
    "    logging_dir=f\"{teacher_output_dir}/logs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "1cf6d906-c1d7-45b6-8e06-54b88ee5fa02",
    "_uuid": "9deff476-a387-4a2f-ad18-1411178c391a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-17T18:00:03.506219Z",
     "iopub.status.busy": "2025-04-17T18:00:03.505941Z",
     "iopub.status.idle": "2025-04-17T18:00:04.160085Z",
     "shell.execute_reply": "2025-04-17T18:00:04.159335Z",
     "shell.execute_reply.started": "2025-04-17T18:00:03.506188Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Trainer for LoRA Teacher Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/487178836.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Teacher Trainer initialized.\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize STANDARD Trainer for LoRA Teacher ---\n",
    "print(\"Initializing Trainer for LoRA Teacher Model...\")\n",
    "trainer = Trainer(\n",
    "    model=model, # Pass the PEFT model (roberta-large + LoRA)\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=teacher_train_dataset, # Use the split train set\n",
    "    eval_dataset=teacher_eval_dataset,   # Use the split validation set\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[metrics_collector],\n",
    ")\n",
    "print(\"LoRA Teacher Trainer initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "263860de-b120-475e-8f82-f603ba5601dc",
    "_uuid": "606d0bb3-de89-46bc-86da-59d59d04f2b9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-17T18:00:06.950285Z",
     "iopub.status.busy": "2025-04-17T18:00:06.949399Z",
     "iopub.status.idle": "2025-04-17T19:40:55.353154Z",
     "shell.execute_reply": "2025-04-17T19:40:55.352236Z",
     "shell.execute_reply.started": "2025-04-17T18:00:06.950250Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting NumPy allowlist...\n",
      "Added NumPy components: ['UInt32DType', '_reconstruct', 'ndarray', 'dtype']\n",
      "Starting LoRA Teacher Model Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3229' max='3229' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3229/3229 1:40:44, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.458300</td>\n",
       "      <td>0.448451</td>\n",
       "      <td>0.952541</td>\n",
       "      <td>0.952533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.436300</td>\n",
       "      <td>0.434302</td>\n",
       "      <td>0.959768</td>\n",
       "      <td>0.959749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.437000</td>\n",
       "      <td>0.431839</td>\n",
       "      <td>0.959507</td>\n",
       "      <td>0.959440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.437800</td>\n",
       "      <td>0.424477</td>\n",
       "      <td>0.962250</td>\n",
       "      <td>0.962219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.425700</td>\n",
       "      <td>0.423432</td>\n",
       "      <td>0.962816</td>\n",
       "      <td>0.962829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.420930</td>\n",
       "      <td>0.963643</td>\n",
       "      <td>0.963626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Teacher training finished.\n",
      "--- LoRA Teacher Training Metrics ---\n",
      "train_runtime: 6047.8523\n",
      "train_samples_per_second: 34.177\n",
      "train_steps_per_second: 0.534\n",
      "total_flos: 3.566797779426816e+16\n",
      "train_loss: 0.46969956109751926\n",
      "epoch: 0.9997677838842015\n",
      "LoRA Teacher Training Duration: 6048.39 seconds\n"
     ]
    }
   ],
   "source": [
    "# --- Train the LoRA Teacher Model ---\n",
    "import torch.serialization; import numpy.core.multiarray\n",
    "try: from numpy.dtypes import UInt32DType\n",
    "except ImportError: UInt32DType = None\n",
    "safe_numpy_globals = []; print(\"Attempting NumPy allowlist...\")\n",
    "try:\n",
    "    safe_numpy_globals.append(np.core.multiarray._reconstruct)\n",
    "    safe_numpy_globals.append(np.ndarray); safe_numpy_globals.append(np.dtype)\n",
    "    if UInt32DType: safe_numpy_globals.append(UInt32DType)\n",
    "    else: safe_numpy_globals.append(np.uint32)\n",
    "    safe_numpy_globals = list(set(safe_numpy_globals))\n",
    "    torch.serialization.add_safe_globals(safe_numpy_globals)\n",
    "    print(f\"Added NumPy components: {[c.__name__ if hasattr(c, '__name__') else str(c) for c in safe_numpy_globals]}\")\n",
    "except Exception as e_gen: print(f\"WARNING: Error setting safe globals for numpy: {e_gen}\")\n",
    "\n",
    "print(\"Starting LoRA Teacher Model Training...\")\n",
    "start_train_time = time.time()\n",
    "try:\n",
    "    # Train fresh\n",
    "    train_result = trainer.train()\n",
    "    print(\"LoRA Teacher training finished.\")\n",
    "    metrics = train_result.metrics\n",
    "    print(\"--- LoRA Teacher Training Metrics ---\")\n",
    "    for key, value in metrics.items(): print(f\"{key}: {value}\")\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "except Exception as e: print(f\"ERROR during teacher training: {e}\"); traceback.print_exc(); raise e\n",
    "end_train_time = time.time()\n",
    "print(f\"LoRA Teacher Training Duration: {end_train_time - start_train_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "16d5621e-252e-42d7-aec3-bc27d6134b85",
    "_uuid": "aa01894f-0939-47b4-ae07-ae7df4ef666e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-17T19:42:47.903968Z",
     "iopub.status.busy": "2025-04-17T19:42:47.903002Z",
     "iopub.status.idle": "2025-04-17T19:42:48.232512Z",
     "shell.execute_reply": "2025-04-17T19:42:48.231811Z",
     "shell.execute_reply.started": "2025-04-17T19:42:47.903939Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving the fine-tuned LoRA teacher adapter to /kaggle/working/roberta_large_lora_teacher_adapter...\n",
      "LoRA Teacher adapter and tokenizer saved to /kaggle/working/roberta_large_lora_teacher_adapter\n",
      "Teacher training arguments saved to /kaggle/working/roberta_large_lora_teacher_adapter/teacher_training_args.json\n"
     ]
    }
   ],
   "source": [
    "# --- Save the Final LoRA Teacher Adapter ---\n",
    "print(f\"\\nSaving the fine-tuned LoRA teacher adapter to {teacher_adapter_save_path}...\")\n",
    "try:\n",
    "    # save_model for PEFT model saves the adapter & config correctly\n",
    "    trainer.save_model(teacher_adapter_save_path)\n",
    "    tokenizer.save_pretrained(teacher_adapter_save_path) # Save tokenizer with adapter\n",
    "    print(f\"LoRA Teacher adapter and tokenizer saved to {teacher_adapter_save_path}\")\n",
    "    # Save training args as well\n",
    "    final_args_path = os.path.join(teacher_adapter_save_path, \"teacher_training_args.json\")\n",
    "    with open(final_args_path, 'w') as f: f.write(training_args.to_json_string())\n",
    "    print(f\"Teacher training arguments saved to {final_args_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR saving final teacher adapter/tokenizer/args: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "472ed542-2f67-41ce-860f-4b3ba3b238bd",
    "_uuid": "b0cea73b-0da4-45a0-a88d-fc2a5809d585",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-17T19:42:51.635623Z",
     "iopub.status.busy": "2025-04-17T19:42:51.635297Z",
     "iopub.status.idle": "2025-04-17T19:47:04.840556Z",
     "shell.execute_reply": "2025-04-17T19:47:04.839859Z",
     "shell.execute_reply.started": "2025-04-17T19:42:51.635601Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running final evaluation on the validation set for the Teacher Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1354' max='718' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [718/718 08:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Teacher Final Validation Metrics ---\n",
      "eval_loss: 0.42092961072921753\n",
      "eval_accuracy: 0.9636434884834763\n",
      "eval_f1: 0.9636262982151969\n",
      "eval_runtime: 253.1916\n",
      "eval_samples_per_second: 90.71\n",
      "eval_steps_per_second: 2.836\n",
      "epoch: 0.9997677838842015\n"
     ]
    }
   ],
   "source": [
    "# --- Optional: Evaluate Teacher on Validation Set ---\n",
    "print(\"\\nRunning final evaluation on the validation set for the Teacher Model...\")\n",
    "try:\n",
    "    eval_metrics = trainer.evaluate(eval_dataset=teacher_eval_dataset)\n",
    "    print(\"--- Teacher Final Validation Metrics ---\")\n",
    "    for key, value in eval_metrics.items(): print(f\"{key}: {value}\")\n",
    "    trainer.save_metrics(\"eval\", eval_metrics)\n",
    "except Exception as e: print(f\"ERROR during final teacher evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-17T20:00:57.566Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- t-SNE / UMAP Visualization ---\n",
    "print(\"\\nSetting up for visualization...\")\n",
    "try:\n",
    "    trainer.model.config.output_hidden_states = True\n",
    "    print(\"Model config set to output hidden states.\")\n",
    "except Exception as e: print(f\"ERROR setting output_hidden_states: {e}\")\n",
    "\n",
    "print(\"Extracting CLS token embeddings for visualization...\")\n",
    "subset_size = 300\n",
    "if len(eval_dataset) < subset_size: subset_size = len(eval_dataset)\n",
    "features = None; label_list = []\n",
    "if subset_size > 0:\n",
    "    try:\n",
    "        subset_indices = random.sample(range(len(eval_dataset)), subset_size)\n",
    "        subset = eval_dataset.select(subset_indices)\n",
    "        feature_list = []\n",
    "        model_device = trainer.model.device\n",
    "        print(f\"Extracting embeddings using device: {model_device}\")\n",
    "        for sample in subset:\n",
    "            input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0).to(model_device)\n",
    "            attention_mask = torch.tensor(sample[\"attention_mask\"]).unsqueeze(0).to(model_device)\n",
    "            with torch.no_grad(): outputs = trainer.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:\n",
    "                 hidden_state = outputs.hidden_states[-1]; cls_embedding = hidden_state[0, 0, :].cpu().numpy()\n",
    "                 feature_list.append(cls_embedding); label_list.append(sample[\"labels\"])\n",
    "            else: print(\"WARNING: Could not get hidden_states for sample.\")\n",
    "        if feature_list: features = np.array(feature_list); print(f\"Extracted {len(features)} embeddings.\")\n",
    "        else: print(\"WARNING: No features were extracted.\")\n",
    "    except Exception as e: print(f\"ERROR extracting embeddings: {e}\")\n",
    "else: print(\"WARNING: Evaluation dataset too small, skipping visualization.\")\n",
    "\n",
    "print(\"Plotting t-SNE\")\n",
    "if features is not None and features.shape[0] > 1:\n",
    "    try:\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, features.shape[0] - 1))\n",
    "        features_2d_tsne = tsne.fit_transform(features); plt.figure(figsize=(8, 6))\n",
    "        scatter = plt.scatter(features_2d_tsne[:, 0], features_2d_tsne[:, 1], c=label_list, cmap=\"viridis\", alpha=0.7)\n",
    "        cbar = plt.colorbar(scatter, label=\"Class Label\", ticks=range(len(class_names))); cbar.ax.set_yticklabels(class_names)\n",
    "        plt.title(\"t-SNE Visualization\"); plt.xlabel(\"t-SNE Comp 1\"); plt.ylabel(\"t-SNE Comp 2\"); plt.show()\n",
    "    except Exception as e: print(f\"ERROR in t-SNE: {e}\")\n",
    "else: print(\"Skipping plots (not enough features extracted).\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7146514,
     "sourceId": 11408837,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7146525,
     "sourceId": 11408863,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
