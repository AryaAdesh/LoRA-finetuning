{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "38f88ff2-9e8b-463e-972c-bb71b448390d",
    "_uuid": "f8a10569-45f1-4605-ab83-ed966b140df5",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Notebook 1: Data Augmentation & Preparation\n",
    "Purpose:\n",
    "1. Load AG News training data.\n",
    "2. Preprocess (tokenize) original data, keeping original text.\n",
    "3. **Run Cleanlab** to identify noisy labels in the training set.\n",
    "4. **Save the cleaned, tokenized ORIGINAL data.**\n",
    "5. Apply efficient, batched Contextual Augmentation to the CLEANED data.\n",
    "6. Tokenize the augmented text.\n",
    "7. Cast labels to ClassLabel type.\n",
    "8. **Save the tokenized AUGMENTED (from cleaned) dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T23:17:29.384343Z",
     "iopub.status.busy": "2025-04-14T23:17:29.384157Z",
     "iopub.status.idle": "2025-04-14T23:17:29.679665Z",
     "shell.execute_reply": "2025-04-14T23:17:29.678634Z",
     "shell.execute_reply.started": "2025-04-14T23:17:29.384324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!rm -rf /kaggle/working/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b8b195a6-3461-4bf6-8856-22c2bd902c4c",
    "_uuid": "4d1783bd-1ce4-4613-bcda-d356c6f55b22",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T23:17:29.680966Z",
     "iopub.status.busy": "2025-04-14T23:17:29.680733Z",
     "iopub.status.idle": "2025-04-14T23:17:29.970895Z",
     "shell.execute_reply": "2025-04-14T23:17:29.970171Z",
     "shell.execute_reply.started": "2025-04-14T23:17:29.680945Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets, ClassLabel, load_from_disk\n",
    "from transformers import RobertaTokenizer, pipeline\n",
    "import traceback\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "import cleanlab\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "f328a810-e7af-437a-9d21-5425c17b009f",
    "_uuid": "ffc05665-b87b-4a02-bb22-5cad5011ce2d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T23:17:29.971876Z",
     "iopub.status.busy": "2025-04-14T23:17:29.971684Z",
     "iopub.status.idle": "2025-04-14T23:17:29.976603Z",
     "shell.execute_reply": "2025-04-14T23:17:29.975944Z",
     "shell.execute_reply.started": "2025-04-14T23:17:29.971861Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "base_model_name = 'roberta-base'\n",
    "dataset_name = 'ag_news'\n",
    "train_split = 'train'\n",
    "\n",
    "# Output path within Kaggle's writable directory\n",
    "processed_data_dir = \"/kaggle/working/processed_data\"\n",
    "# --- Define TWO separate save paths ---\n",
    "cleaned_original_save_path = os.path.join(processed_data_dir, \"cleaned_tokenized_original_agnews\")\n",
    "tokenized_augmented_save_path = os.path.join(processed_data_dir, \"tokenized_augmented_from_cleaned_agnews\") # New name\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "\n",
    "# Augmentation settings\n",
    "AUGMENTATION_MASK_PROBABILITY = 0.2\n",
    "AUGMENTATION_MAP_BATCH_SIZE = 256\n",
    "AUGMENTATION_TOP_K = 1\n",
    "\n",
    "# Tokenization settings\n",
    "TOKENIZER_MAX_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "6dc7ee9b-8fef-4177-a431-31c3019abb6c",
    "_uuid": "12b6d02c-09cd-437e-a17f-b7db426a4559",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T23:17:29.978763Z",
     "iopub.status.busy": "2025-04-14T23:17:29.978353Z",
     "iopub.status.idle": "2025-04-14T23:17:37.186859Z",
     "shell.execute_reply": "2025-04-14T23:17:37.186242Z",
     "shell.execute_reply.started": "2025-04-14T23:17:29.978745Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: CUDA available. Initializing pipeline on device 0.\n",
      "INFO: GPU Name: Tesla P100-PCIE-16GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c63ce09dfa4eebbec19fbe6a6a11d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c403b0acfe5f42548ba1920530c16741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c8b5296f8c43b3891c2ec3ab46c6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd98e9481dca4b5691cf9f6501f7f887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d534822fc5492e99d7cc17a57e2a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f73355774c4246abfd3c471069671f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Initialized fill-mask pipeline on device: cuda:0\n",
      "INFO: Mask token: <mask>\n"
     ]
    }
   ],
   "source": [
    "# --- GPU Check and Pipeline Initialization ---\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"WARNING: CUDA (GPU) is not available. Pipeline will run on CPU (will be slow).\")\n",
    "    pipeline_device = -1\n",
    "else:\n",
    "    pipeline_device = 0\n",
    "    print(f\"INFO: CUDA available. Initializing pipeline on device {pipeline_device}.\")\n",
    "    print(f\"INFO: GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Initialize the fill-mask pipeline\n",
    "try:\n",
    "    fill_mask_pipeline = pipeline(\n",
    "        \"fill-mask\", model=base_model_name, tokenizer=base_model_name, device=pipeline_device\n",
    "    )\n",
    "    print(f\"INFO: Initialized fill-mask pipeline on device: {fill_mask_pipeline.device}\")\n",
    "    mask_token = fill_mask_pipeline.tokenizer.mask_token\n",
    "    print(f\"INFO: Mask token: {mask_token}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to initialize fill-mask pipeline: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "4df2837b-8239-49ba-b012-7d8e995a3a92",
    "_uuid": "6a931930-aec1-4888-8f17-04d0f108b658",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T23:17:37.187859Z",
     "iopub.status.busy": "2025-04-14T23:17:37.187597Z",
     "iopub.status.idle": "2025-04-14T23:17:37.522963Z",
     "shell.execute_reply": "2025-04-14T23:17:37.522295Z",
     "shell.execute_reply.started": "2025-04-14T23:17:37.187832Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Loading tokenizer: roberta-base\n"
     ]
    }
   ],
   "source": [
    "# --- Load Tokenizer ---\n",
    "print(f\"INFO: Loading tokenizer: {base_model_name}\")\n",
    "try:\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(base_model_name)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load tokenizer {base_model_name}: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "a5c4ca09-56b5-4602-82cc-a78b2f1ebeb6",
    "_uuid": "fa96b1ca-2e8c-4bf3-a55c-517c5be7142e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T23:17:44.609842Z",
     "iopub.status.busy": "2025-04-14T23:17:44.609544Z",
     "iopub.status.idle": "2025-04-14T23:18:06.726963Z",
     "shell.execute_reply": "2025-04-14T23:18:06.726384Z",
     "shell.execute_reply.started": "2025-04-14T23:17:44.609811Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Loading dataset 'ag_news', split 'train'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86200db3dea473595559156848a24eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d257de4a758a4461bf4765fc2741ea45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82a7ea2236348b191a150a70a25947c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b09666668124eaa80451e53d9c2f7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c28990ed8e418698854ba2bfd6c9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Original dataset loaded with 120000 examples.\n",
      "INFO: Original features: {'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['World', 'Sports', 'Business', 'Sci/Tech'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "# --- Load Original Dataset ---\n",
    "print(f\"INFO: Loading dataset '{dataset_name}', split '{train_split}'...\")\n",
    "try:\n",
    "    original_dataset = load_dataset(dataset_name, split=train_split)\n",
    "    print(f\"INFO: Original dataset loaded with {len(original_dataset)} examples.\")\n",
    "    print(f\"INFO: Original features: {original_dataset.features}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load dataset: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "0a6bdd1b-73ab-4073-9a7f-55b23f2b3769",
    "_uuid": "f4894b5d-8b0c-4834-8d3b-2d9d2ef33c2b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T23:18:06.728260Z",
     "iopub.status.busy": "2025-04-14T23:18:06.727960Z",
     "iopub.status.idle": "2025-04-14T23:18:35.457513Z",
     "shell.execute_reply": "2025-04-14T23:18:35.456524Z",
     "shell.execute_reply.started": "2025-04-14T23:18:06.728243Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Starting initial tokenization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e5f1f8a49a49109112850d33b6cbe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Initial tokenization complete.\n",
      "INFO: Processed train dataset columns: ['labels', 'input_ids', 'attention_mask', 'orig_text']\n",
      "INFO: Stored original ClassLabel features: ClassLabel(names=['World', 'Sports', 'Business', 'Sci/Tech'], id=None)\n"
     ]
    }
   ],
   "source": [
    "# --- Initial Preprocessing (Tokenize + Keep Original Text) ---\n",
    "def preprocess_initial(examples):\n",
    "    tokenized = tokenizer(examples['text'], truncation=True, padding=False, max_length=TOKENIZER_MAX_LENGTH)\n",
    "    tokenized['orig_text'] = examples['text']\n",
    "    return tokenized\n",
    "\n",
    "print(\"INFO: Starting initial tokenization...\")\n",
    "num_cpus = os.cpu_count()\n",
    "num_proc_initial = max(1, num_cpus - 2) if num_cpus > 2 else 1\n",
    "try:\n",
    "    # This dataset contains all original examples, tokenized + orig_text\n",
    "    train_dataset_processed = original_dataset.map(\n",
    "        preprocess_initial, batched=True, num_proc=num_proc_initial, remove_columns=['text']\n",
    "    )\n",
    "    if 'label' in train_dataset_processed.column_names:\n",
    "        train_dataset_processed = train_dataset_processed.rename_column(\"label\", \"labels\")\n",
    "    print(\"INFO: Initial tokenization complete.\")\n",
    "    print(f\"INFO: Processed train dataset columns: {train_dataset_processed.column_names}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Error during initial preprocessing: {e}\")\n",
    "    raise e\n",
    "\n",
    "try:\n",
    "    # Store feature info from the full processed dataset before cleaning\n",
    "    original_labels_feature = train_dataset_processed.features['labels']\n",
    "    if not isinstance(original_labels_feature, ClassLabel):\n",
    "        print(f\"WARNING: Original 'labels' feature is not ClassLabel: {original_labels_feature}.\")\n",
    "    else:\n",
    "        print(f\"INFO: Stored original ClassLabel features: {original_labels_feature}\")\n",
    "except KeyError:\n",
    "    print(\"ERROR: 'labels' column not found after preprocessing.\")\n",
    "    raise KeyError(\"'labels' column missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "d12d9594-5b2c-4b64-80cc-59e8667feb9a",
    "_uuid": "84f2d4e4-0026-48dc-898f-f86858c4ecf2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T23:18:38.806090Z",
     "iopub.status.busy": "2025-04-14T23:18:38.805760Z",
     "iopub.status.idle": "2025-04-14T23:19:55.958452Z",
     "shell.execute_reply": "2025-04-14T23:19:55.957528Z",
     "shell.execute_reply.started": "2025-04-14T23:18:38.806037Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Preparing data for Cleanlab...\n",
      "INFO: Using 120000 training texts for cleanlab analysis.\n",
      "INFO: Loading Sentence Transformer model for embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd5c62bc2e64c69930eaf41787d68d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87954f1ef6d49c4ad6fcbd7e6a4468b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326ce0ad59734ff98a95ab97a2a97478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adae44248c924b0a8815a96e8434988f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78a731154474ec4b902d8891036b6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967c2c22c7b34ee38de97a73758e1f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da939c8fb9e3412286b64390fdb8e911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009105a8992948f0adca5eacc1757e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1495cfb584724052b2298a46b4f3d534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a343e4008d1c4ced9dcd0b6c0c9d1753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd088f97a96a4270990d266591375fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Encoding training texts on cuda...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b3c94738184d9b8204578ba6d1262c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Generated embeddings of shape: (120000, 384)\n"
     ]
    }
   ],
   "source": [
    "# --- Cleanlab: Generate Embeddings ---\n",
    "print(\"INFO: Preparing data for Cleanlab...\")\n",
    "try:\n",
    "    train_texts = train_dataset_processed[\"orig_text\"]\n",
    "    train_labels_np = np.array(train_dataset_processed[\"labels\"])\n",
    "    print(f\"INFO: Using {len(train_texts)} training texts for cleanlab analysis.\")\n",
    "except KeyError as e:\n",
    "    print(f\"ERROR: Missing 'orig_text' or 'labels' column: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"INFO: Loading Sentence Transformer model for embeddings...\")\n",
    "try:\n",
    "    sbert_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    sbert_model = SentenceTransformer('all-MiniLM-L6-v2', device=sbert_device)\n",
    "    print(f\"INFO: Encoding training texts on {sbert_device}...\")\n",
    "    train_embeddings = sbert_model.encode(train_texts, show_progress_bar=True, batch_size=128)\n",
    "    print(f\"INFO: Generated embeddings of shape: {train_embeddings.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to generate sentence embeddings: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "861f1b1d-983a-40e5-b4a4-e6775caed5b5",
    "_uuid": "4160792e-771c-4287-b189-e58e6f2f8281",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T23:19:55.960160Z",
     "iopub.status.busy": "2025-04-14T23:19:55.959907Z",
     "iopub.status.idle": "2025-04-14T23:21:39.896997Z",
     "shell.execute_reply": "2025-04-14T23:21:39.896158Z",
     "shell.execute_reply.started": "2025-04-14T23:19:55.960141Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Getting out-of-sample predicted probabilities via cross-validation...\n",
      "INFO: Generated out-of-sample probabilities of shape: (120000, 4)\n"
     ]
    }
   ],
   "source": [
    "# --- Cleanlab: Get Out-of-Sample Predicted Probabilities (on CPU) ---\n",
    "print(\"INFO: Getting out-of-sample predicted probabilities via cross-validation...\")\n",
    "try:\n",
    "    classifier = LogisticRegression(solver='liblinear', random_state=42)\n",
    "    num_cv_folds = 5\n",
    "    pred_probs = cross_val_predict(\n",
    "        classifier, train_embeddings, train_labels_np,\n",
    "        cv=StratifiedKFold(num_cv_folds, shuffle=True, random_state=42),\n",
    "        method='predict_proba', n_jobs=num_proc_initial\n",
    "    )\n",
    "    print(f\"INFO: Generated out-of-sample probabilities of shape: {pred_probs.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed during cross-validation prediction: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "314ffb0f-d61f-4b80-838d-b1cb30a2efbc",
    "_uuid": "2802e5ca-db56-4b2e-b8da-a61a8280e2aa",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T23:21:54.675367Z",
     "iopub.status.busy": "2025-04-14T23:21:54.675041Z",
     "iopub.status.idle": "2025-04-14T23:21:54.967719Z",
     "shell.execute_reply": "2025-04-14T23:21:54.966679Z",
     "shell.execute_reply.started": "2025-04-14T23:21:54.675342Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Running cleanlab to find label issues...\n",
      "INFO: Cleanlab found 5168 potential label issues.\n",
      "INFO: Example issue indices: [ 67035  66633 107558  19150  63316   4307  96061 101562  44984  47030]...\n",
      "INFO: Identified 114832 examples with likely clean labels.\n"
     ]
    }
   ],
   "source": [
    "# --- Cleanlab: Find Label Issues ---\n",
    "print(\"INFO: Running cleanlab to find label issues...\")\n",
    "try:\n",
    "    label_issues_info = cleanlab.filter.find_label_issues(\n",
    "        labels=train_labels_np, pred_probs=pred_probs, return_indices_ranked_by='self_confidence'\n",
    "    )\n",
    "    num_issues_found = len(label_issues_info)\n",
    "    print(f\"INFO: Cleanlab found {num_issues_found} potential label issues.\")\n",
    "    if num_issues_found > 0: print(f\"INFO: Example issue indices: {label_issues_info[:10]}...\")\n",
    "    issue_indices = label_issues_info\n",
    "    all_indices = np.arange(len(train_labels_np))\n",
    "    indices_to_keep = np.setdiff1d(all_indices, issue_indices, assume_unique=True)\n",
    "    print(f\"INFO: Identified {len(indices_to_keep)} examples with likely clean labels.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed during cleanlab issue finding: {e}\")\n",
    "    print(\"WARNING: Proceeding without cleaning due to error.\")\n",
    "    indices_to_keep = np.arange(len(train_labels_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "6aaf5e25-605a-4d19-a999-f21136052fa6",
    "_uuid": "44eefc8c-e24f-4bb5-baeb-6ec8d530e40e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T23:22:00.120909Z",
     "iopub.status.busy": "2025-04-14T23:22:00.120132Z",
     "iopub.status.idle": "2025-04-14T23:22:00.765790Z",
     "shell.execute_reply": "2025-04-14T23:22:00.765035Z",
     "shell.execute_reply.started": "2025-04-14T23:22:00.120872Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Creating cleaned dataset by selecting 114832 examples...\n",
      "INFO: Cleaned training dataset size: 114832\n"
     ]
    }
   ],
   "source": [
    "# --- Filter Dataset ---\n",
    "print(f\"INFO: Creating cleaned dataset by selecting {len(indices_to_keep)} examples...\")\n",
    "# This dataset contains the CLEANED ORIGINAL examples, tokenized, with orig_text\n",
    "cleaned_train_dataset_processed = train_dataset_processed.select(indices_to_keep)\n",
    "print(f\"INFO: Cleaned training dataset size: {len(cleaned_train_dataset_processed)}\")\n",
    "\n",
    "# Clean up memory from cleanlab step\n",
    "del train_embeddings, pred_probs, label_issues_info, train_labels_np, train_texts\n",
    "gc.collect()\n",
    "if torch.cuda.is_available(): torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "0fdb2c37-7665-474f-b0a9-4ac67e8ffef8",
    "_uuid": "5510a7c1-6570-4bb4-8618-4375980268ef",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T23:22:16.580403Z",
     "iopub.status.busy": "2025-04-14T23:22:16.579849Z",
     "iopub.status.idle": "2025-04-14T23:22:17.373389Z",
     "shell.execute_reply": "2025-04-14T23:22:17.372809Z",
     "shell.execute_reply.started": "2025-04-14T23:22:16.580379Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Saving CLEANED, TOKENIZED ORIGINAL data to /kaggle/working/processed_data/cleaned_tokenized_original_agnews...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d077ff8f49145b58bdf842498ae180d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/114832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee37b6c889b4e489cef6e831c7ef064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/114832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Cleaned, tokenized original data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "print(f\"INFO: Saving CLEANED, TOKENIZED ORIGINAL data to {cleaned_original_save_path}...\")\n",
    "try:\n",
    "    required_columns_save = ['input_ids', 'attention_mask', 'labels']\n",
    "    if not all(col in cleaned_train_dataset_processed.column_names for col in required_columns_save):\n",
    "         raise KeyError(f\"Cleaned dataset missing columns for saving. Found: {cleaned_train_dataset_processed.column_names}\")\n",
    "\n",
    "    cleaned_to_save = cleaned_train_dataset_processed.select_columns(required_columns_save)\n",
    "    # Ensure labels feature is correct before saving\n",
    "    cleaned_to_save = cleaned_to_save.cast_column('labels', original_labels_feature)\n",
    "\n",
    "    cleaned_to_save.save_to_disk(cleaned_original_save_path)\n",
    "    print(\"INFO: Cleaned, tokenized original data saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Error saving cleaned original data: {e}\")\n",
    "    traceback.print_exc()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "7865d5ff-1ed6-43a1-8f18-46638b247527",
    "_uuid": "a341a289-b7c3-403c-8ba0-7ba8a9c34584",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T23:22:22.545906Z",
     "iopub.status.busy": "2025-04-14T23:22:22.545618Z",
     "iopub.status.idle": "2025-04-14T23:22:22.559894Z",
     "shell.execute_reply": "2025-04-14T23:22:22.559341Z",
     "shell.execute_reply.started": "2025-04-14T23:22:22.545885Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Batched contextual augmentation function defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Optimized Batched Contextual Augmentation Function ---\n",
    "def augment_contextual_batch_batched(examples):\n",
    "    \"\"\" Performs contextual augmentation efficiently by batching pipeline calls. \"\"\"\n",
    "    global fill_mask_pipeline, mask_token\n",
    "    input_texts = examples[\"orig_text\"]\n",
    "    batch_size = len(input_texts);\n",
    "    if batch_size == 0: return {\"text_augmented\": []}\n",
    "    masked_texts_info = []; original_texts_for_fallback = {}\n",
    "    # 1. Prepare masked inputs\n",
    "    for i, text in enumerate(input_texts):\n",
    "        original_texts_for_fallback[i] = text; words = text.split()\n",
    "        if not words: continue\n",
    "        new_words = words.copy(); masked = False; num_masks = 0; max_masks = 5; indices_masked = []\n",
    "        word_indices = list(range(len(words))); random.shuffle(word_indices)\n",
    "        for idx in word_indices:\n",
    "            if random.random() < AUGMENTATION_MASK_PROBABILITY and num_masks < max_masks:\n",
    "                 if len(words[idx]) > 1 and not words[idx].startswith('<'):\n",
    "                    new_words[idx] = mask_token; indices_masked.append(idx); masked = True; num_masks += 1\n",
    "        if not masked and len(words) > 0:\n",
    "             attempts = 0\n",
    "             while attempts < 5:\n",
    "                 idx = random.randint(0, len(words) - 1)\n",
    "                 if len(words[idx]) > 1 and not words[idx].startswith('<'):\n",
    "                     new_words[idx] = mask_token; indices_masked.append(idx); masked = True; num_masks += 1; break\n",
    "                 attempts += 1\n",
    "        if masked: masked_text = \" \".join(new_words); masked_texts_info.append( (i, masked_text, words, sorted(indices_masked)) )\n",
    "    if not masked_texts_info: return {\"text_augmented\": input_texts}\n",
    "    batch_masked_texts = [info[1] for info in masked_texts_info]\n",
    "    # 2. Single Pipeline Call\n",
    "    try: pipeline_results = fill_mask_pipeline(batch_masked_texts, top_k=AUGMENTATION_TOP_K)\n",
    "    except Exception as e: print(f\"ERROR: Fill-Mask Pipeline error: {e}. Returning originals.\"); return {\"text_augmented\": input_texts}\n",
    "    # 3. Process results\n",
    "    final_augmented_texts = [\"\"] * batch_size; processed_indices = set(info[0] for info in masked_texts_info)\n",
    "    for i in range(batch_size):\n",
    "        if i not in processed_indices: final_augmented_texts[i] = original_texts_for_fallback[i]\n",
    "    if len(pipeline_results) == len(masked_texts_info):\n",
    "        for result_idx, original_info in enumerate(masked_texts_info):\n",
    "            original_index, original_words, masked_indices = original_info[0], original_info[2], original_info[3]\n",
    "            current_predictions = pipeline_results[result_idx]; reconstructed_words = original_words[:]\n",
    "            if not current_predictions: final_augmented_texts[original_index] = \" \".join(reconstructed_words); continue\n",
    "            num_masks_in_input = len(masked_indices); num_pred_groups = 0\n",
    "            if isinstance(current_predictions, list) and current_predictions:\n",
    "                if isinstance(current_predictions[0], list): num_pred_groups = len(current_predictions)\n",
    "                elif isinstance(current_predictions[0], dict): num_pred_groups = 1\n",
    "            if num_pred_groups == num_masks_in_input:\n",
    "                pred_idx = 0\n",
    "                for target_idx in masked_indices:\n",
    "                    token_str = \"\"; prediction_item = None\n",
    "                    try:\n",
    "                        if isinstance(current_predictions[0], list):\n",
    "                            if pred_idx < len(current_predictions) and current_predictions[pred_idx]: prediction_item = current_predictions[pred_idx][0]\n",
    "                        else:\n",
    "                             if pred_idx == 0 and current_predictions: prediction_item = current_predictions[0]\n",
    "                    except IndexError: print(f\"WARNING: IndexError accessing prediction {pred_idx}\")\n",
    "                    if prediction_item and isinstance(prediction_item, dict) and 'token_str' in prediction_item: token_str = prediction_item['token_str'].strip()\n",
    "                    if token_str and not token_str.isspace() and target_idx < len(reconstructed_words): reconstructed_words[target_idx] = token_str\n",
    "                    pred_idx += 1\n",
    "                final_augmented_texts[original_index] = \" \".join(reconstructed_words)\n",
    "            else: print(f\"WARNING: Mask/pred mismatch item {original_index}. Using original.\"); final_augmented_texts[original_index] = \" \".join(original_words)\n",
    "    else: print(f\"ERROR: Mismatch #masked vs #results. Reverting batch.\"); final_augmented_texts = [original_texts_for_fallback.get(i, \"\") for i in range(batch_size)]\n",
    "    return {\"text_augmented\": final_augmented_texts}\n",
    "\n",
    "print(\"INFO: Batched contextual augmentation function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "345a9f05-f9ea-49b4-b26e-a86aea253f47",
    "_uuid": "fe941840-0cf4-403f-b650-1f2bfc59ea4e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T23:22:26.965712Z",
     "iopub.status.busy": "2025-04-14T23:22:26.965136Z",
     "iopub.status.idle": "2025-04-14T23:47:54.351701Z",
     "shell.execute_reply": "2025-04-14T23:47:54.350921Z",
     "shell.execute_reply.started": "2025-04-14T23:22:26.965685Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Starting batched contextual augmentation mapping for CLEANED data (using GPU)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75924e25feb64211b0ffe542a17b12a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/114832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Contextual augmentation mapping finished in 1527.38 seconds.\n",
      "INFO: Columns after map: ['labels', 'input_ids', 'attention_mask', 'orig_text', 'text_augmented']\n"
     ]
    }
   ],
   "source": [
    "# --- Apply ONLY Batched Contextual Augmentation (ON CLEANED DATA) ---\n",
    "print(\"INFO: Starting batched contextual augmentation mapping for CLEANED data (using GPU)...\")\n",
    "start_time_ctx = time.time()\n",
    "try:\n",
    "    # Apply augmentation to the CLEANED dataset\n",
    "    augmented_train_dataset_features = cleaned_train_dataset_processed.map(\n",
    "        augment_contextual_batch_batched, # The efficient batched function\n",
    "        batched=True,\n",
    "        batch_size=AUGMENTATION_MAP_BATCH_SIZE,\n",
    "        num_proc=1, # Must be 1 for GPU safety\n",
    "    )\n",
    "    end_time_ctx = time.time()\n",
    "    print(f\"INFO: Contextual augmentation mapping finished in {end_time_ctx - start_time_ctx:.2f} seconds.\")\n",
    "    print(f\"INFO: Columns after map: {augmented_train_dataset_features.column_names}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Error during contextual augmentation mapping: {e}\")\n",
    "    traceback.print_exc()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "1abf3f4d-dcad-4f2d-9186-3f313d7ea26f",
    "_uuid": "443318ae-3a4f-4898-a89e-015671445f23",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T23:49:57.961343Z",
     "iopub.status.busy": "2025-04-14T23:49:57.960604Z",
     "iopub.status.idle": "2025-04-14T23:49:58.425626Z",
     "shell.execute_reply": "2025-04-14T23:49:58.425009Z",
     "shell.execute_reply.started": "2025-04-14T23:49:57.961317Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Creating dataset from augmented text...\n",
      "INFO: Augmented-only dataset created.\n"
     ]
    }
   ],
   "source": [
    "# --- Create Augmented-Only Dataset ---\n",
    "# This dataset contains the AUGMENTED text generated from CLEANED originals, plus labels\n",
    "print(\"INFO: Creating dataset from augmented text...\")\n",
    "try:\n",
    "    required_cols_after_map = ['labels', 'text_augmented']\n",
    "    if not all(col in augmented_train_dataset_features.column_names for col in required_cols_after_map):\n",
    "         raise KeyError(f\"Map output missing required columns. Found: {augmented_train_dataset_features.column_names}\")\n",
    "\n",
    "    augmented_only_dataset = Dataset.from_dict({\n",
    "        \"text\": augmented_train_dataset_features[\"text_augmented\"],\n",
    "        \"labels\": augmented_train_dataset_features[\"labels\"]\n",
    "    })\n",
    "    print(f\"INFO: Augmented-only dataset created.\")\n",
    "except KeyError as e:\n",
    "     print(f\"ERROR: Column error creating final augmented dataset: {e}\")\n",
    "     raise e\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Error creating dataset from dict: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "b9645973-6683-4cd5-880b-3b30754dc35d",
    "_uuid": "5e7749b5-39a1-4fc9-80a9-ca319e5c19cc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T23:50:04.368309Z",
     "iopub.status.busy": "2025-04-14T23:50:04.367592Z",
     "iopub.status.idle": "2025-04-14T23:50:31.477040Z",
     "shell.execute_reply": "2025-04-14T23:50:31.476215Z",
     "shell.execute_reply.started": "2025-04-14T23:50:04.368283Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Tokenizing augmented text dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4148b875174249914d37746b67e63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/114832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Tokenization of augmented text complete in 27.10 seconds.\n",
      "INFO: Tokenized augmented dataset columns: ['labels', 'input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# --- Tokenize Augmented Text ---\n",
    "def preprocess_augmented_text(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=False, max_length=TOKENIZER_MAX_LENGTH)\n",
    "\n",
    "print(\"INFO: Tokenizing augmented text dataset...\")\n",
    "start_time_tok = time.time()\n",
    "num_proc_tokenize = max(1, os.cpu_count() - 2) if os.cpu_count() > 2 else 1\n",
    "try:\n",
    "    tokenized_augmented_dataset = augmented_only_dataset.map(\n",
    "        preprocess_augmented_text, batched=True, remove_columns=[\"text\"], num_proc=num_proc_tokenize\n",
    "    )\n",
    "    end_time_tok = time.time()\n",
    "    print(f\"INFO: Tokenization of augmented text complete in {end_time_tok - start_time_tok:.2f} seconds.\")\n",
    "    print(f\"INFO: Tokenized augmented dataset columns: {tokenized_augmented_dataset.column_names}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Error during augmented text tokenization: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "df0339c7-3015-4ab8-8590-a2107a33f514",
    "_uuid": "d9178cde-4577-4373-ba24-4c62c3fef317",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T23:50:35.011023Z",
     "iopub.status.busy": "2025-04-14T23:50:35.010429Z",
     "iopub.status.idle": "2025-04-14T23:50:35.105981Z",
     "shell.execute_reply": "2025-04-14T23:50:35.105098Z",
     "shell.execute_reply.started": "2025-04-14T23:50:35.010995Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Casting labels column type for augmented data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2366f87acd754d8bbb9b8a0099718bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/114832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Augmented dataset labels feature after cast: ClassLabel(names=['World', 'Sports', 'Business', 'Sci/Tech'], id=None)\n"
     ]
    }
   ],
   "source": [
    "# --- Cast Labels Column for Augmented Data---\n",
    "print(\"INFO: Casting labels column type for augmented data...\")\n",
    "try:\n",
    "    if 'original_labels_feature' not in globals(): raise NameError(\"original_labels_feature not defined.\")\n",
    "    tokenized_augmented_dataset = tokenized_augmented_dataset.cast_column(\n",
    "        'labels', original_labels_feature\n",
    "    )\n",
    "    print(f\"INFO: Augmented dataset labels feature after cast: {tokenized_augmented_dataset.features['labels']}\")\n",
    "except NameError as e: print(f\"ERROR: Original label features variable not found: {e}\"); raise\n",
    "except Exception as e: print(f\"ERROR: Error casting labels column: {e}\"); raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "b1341396-6e43-4ac8-bba9-0a2a315de2d6",
    "_uuid": "f8eae0a6-579e-4caa-b576-1e94e7409028",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T23:50:38.111438Z",
     "iopub.status.busy": "2025-04-14T23:50:38.111162Z",
     "iopub.status.idle": "2025-04-14T23:50:38.213651Z",
     "shell.execute_reply": "2025-04-14T23:50:38.213067Z",
     "shell.execute_reply.started": "2025-04-14T23:50:38.111419Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Saving tokenized AUGMENTED (from cleaned) data to /kaggle/working/processed_data/tokenized_augmented_from_cleaned_agnews...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31488ac99e6546e4a4b9b6aa1ff6c192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/114832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Tokenized augmented dataset saved successfully.\n",
      "INFO: Output files should contain TWO directories now:\n",
      "      1. /kaggle/working/processed_data/cleaned_tokenized_original_agnews\n",
      "      2. /kaggle/working/processed_data/tokenized_augmented_from_cleaned_agnews\n"
     ]
    }
   ],
   "source": [
    "# --- Save Processed AUGMENTED Dataset ---\n",
    "print(f\"INFO: Saving tokenized AUGMENTED (from cleaned) data to {tokenized_augmented_save_path}...\")\n",
    "try:\n",
    "    # Ensure final columns are correct before saving\n",
    "    tokenized_augmented_dataset = tokenized_augmented_dataset.select_columns(['input_ids', 'attention_mask', 'labels'])\n",
    "    tokenized_augmented_dataset.save_to_disk(tokenized_augmented_save_path)\n",
    "    print(\"INFO: Tokenized augmented dataset saved successfully.\")\n",
    "    print(f\"INFO: Output files should contain TWO directories now:\\n\"\n",
    "          f\"      1. {cleaned_original_save_path}\\n\"\n",
    "          f\"      2. {tokenized_augmented_save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Error saving tokenized augmented dataset to disk: {e}\")\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
