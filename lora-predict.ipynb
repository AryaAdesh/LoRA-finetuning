{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "61a774e8-19d1-4e28-ae79-119b48cd98b1",
    "_uuid": "b3c3152c-21a0-4e3b-b82c-0dcdc96ca428",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Notebook 3: Prediction (GPU - Using Saved LoRA Adapter)\n",
    "Purpose:\n",
    "1. Load tokenizer and LoRA adapter files saved by Notebook 2.\n",
    "2. Load the base RoBERTa model architecture and weights from Hugging Face Hub.\n",
    "3. Apply the saved LoRA adapter to the base model.\n",
    "4. Load and preprocess the unlabeled test data.\n",
    "5. Run inference on the GPU using a manual prediction loop.\n",
    "6. Generate the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "3cf6d141-396c-44cd-b921-5b9b45845c4f",
    "_uuid": "b742d617-ae00-40cb-8c48-bae39e26ba8e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-19T03:36:31.770257Z",
     "iopub.status.busy": "2025-04-19T03:36:31.770026Z",
     "iopub.status.idle": "2025-04-19T03:36:58.162228Z",
     "shell.execute_reply": "2025-04-19T03:36:58.161483Z",
     "shell.execute_reply.started": "2025-04-19T03:36:31.770232Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 03:36:47.295915: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745033807.476266      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745033807.527173      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# --- Essential Imports ---\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    AutoConfig \n",
    ")\n",
    "from peft import PeftModel, PeftConfig \n",
    "from tqdm.auto import tqdm \n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "a8d9faf3-f810-4dea-a29f-a9c7d773fcdb",
    "_uuid": "c7945ac6-f232-40a1-8e28-0eef4dc2c576",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-19T03:36:58.164509Z",
     "iopub.status.busy": "2025-04-19T03:36:58.163749Z",
     "iopub.status.idle": "2025-04-19T03:36:58.168780Z",
     "shell.execute_reply": "2025-04-19T03:36:58.168101Z",
     "shell.execute_reply.started": "2025-04-19T03:36:58.164487Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "saved_model_path = \"/kaggle/input/alt/transformers/default/1\"\n",
    "\n",
    "# 2. Path to the original unlabeled test data pickle file\n",
    "unlabeled_data_path = \"/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl\"\n",
    "\n",
    "# --- Model & Tokenizer Settings ---\n",
    "base_model_name = 'roberta-base'\n",
    "TOKENIZER_MAX_LENGTH = 512\n",
    "\n",
    "# --- Prediction Settings ---\n",
    "PREDICTION_BATCH_SIZE = 16 #\n",
    "\n",
    "num_labels = 4\n",
    "id2label = {0: 'World', 1: 'Sports', 2: 'Business', 3: 'Sci/Tech'}\n",
    "label2id = {'World': 0, 'Sports': 1, 'Business': 2, 'Sci/Tech': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "be893b5f-fb70-4455-b0f6-9b3874986694",
    "_uuid": "88ba0031-ecf5-40b0-931c-cc7834c06580",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-19T03:36:58.170297Z",
     "iopub.status.busy": "2025-04-19T03:36:58.169713Z",
     "iopub.status.idle": "2025-04-19T03:36:58.188086Z",
     "shell.execute_reply": "2025-04-19T03:36:58.187449Z",
     "shell.execute_reply.started": "2025-04-19T03:36:58.170279Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. Using device: cuda\n",
      "GPU Name: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "# --- GPU Check ---\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU is available. Using device: {device}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"WARNING: GPU not available, using CPU. Prediction will be very slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "294e03b0-7f30-4e7e-973a-376275bd7fda",
    "_uuid": "21028783-9804-40a2-9de3-26cf0aa84db5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-19T03:36:58.189188Z",
     "iopub.status.busy": "2025-04-19T03:36:58.188827Z",
     "iopub.status.idle": "2025-04-19T03:36:58.364139Z",
     "shell.execute_reply": "2025-04-19T03:36:58.363388Z",
     "shell.execute_reply.started": "2025-04-19T03:36:58.189170Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from saved path: /kaggle/input/alt/transformers/default/1\n",
      "Tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Tokenizer ---\n",
    "print(f\"Loading tokenizer from saved path: {saved_model_path}\")\n",
    "try:\n",
    "    # Check if path exists before loading\n",
    "    if not os.path.isdir(saved_model_path):\n",
    "         raise FileNotFoundError(f\"Directory not found: {saved_model_path}\")\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(saved_model_path)\n",
    "    print(\"Tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load tokenizer from {saved_model_path}: {e}\")\n",
    "    print(f\"Ensure the path is correct and contains tokenizer files (vocab.json, merges.txt etc.)\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "7af28b7c-ee16-4a26-bf5f-889207dc7a79",
    "_uuid": "ca6dc44a-aa12-44c9-9a42-a0924ecb2d1f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-19T03:36:58.365964Z",
     "iopub.status.busy": "2025-04-19T03:36:58.365643Z",
     "iopub.status.idle": "2025-04-19T03:37:01.584855Z",
     "shell.execute_reply": "2025-04-19T03:37:01.584200Z",
     "shell.execute_reply.started": "2025-04-19T03:36:58.365925Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Model Loading Process ---\n",
      "Loading base config for roberta-base from Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d9eb32efcf45bab76b55daaf7ce7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base config loaded and updated with label info.\n",
      "Loading base model weights for roberta-base from Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca11fff95da439cb421abfcfd303c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model architecture and weights loaded.\n",
      "Loading LoRA adapter weights from: /kaggle/input/alt/transformers/default/1\n",
      "PEFT adapter loaded and applied to base model.\n",
      "Merging LoRA adapters into base model for faster inference...\n",
      "LoRA adapters merged and unloaded.\n",
      "Model moved to cuda and set to evaluation mode.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Starting Model Loading Process ---\")\n",
    "try:\n",
    "    # 1. Load base config from HUB, add label info manually\n",
    "    print(f\"Loading base config for {base_model_name} from Hub...\")\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        base_model_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    print(\"Base config loaded and updated with label info.\")\n",
    "\n",
    "    # 2. Load base model weights from HUB, using updated config\n",
    "    print(f\"Loading base model weights for {base_model_name} from Hub...\")\n",
    "    base_model = RobertaForSequenceClassification.from_pretrained(\n",
    "        base_model_name,\n",
    "        config=config,\n",
    "    )\n",
    "    print(\"Base model architecture and weights loaded.\")\n",
    "\n",
    "    # 3. Load LoRA adapter weights from the SAVED PATH and apply to base model\n",
    "    print(f\"Loading LoRA adapter weights from: {saved_model_path}\")\n",
    "\n",
    "    model = PeftModel.from_pretrained(base_model, saved_model_path)\n",
    "    print(\"PEFT adapter loaded and applied to base model.\")\n",
    "\n",
    "    print(\"Merging LoRA adapters into base model for faster inference...\")\n",
    "    model = model.merge_and_unload()\n",
    "    print(\"LoRA adapters merged and unloaded.\")\n",
    "\n",
    "    # 5. Move model to device and set to evaluation mode\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Model moved to {device} and set to evaluation mode.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during model loading: {e}\")\n",
    "    print(\"Check paths, model names, and ensure necessary files (adapter_*, base model cache) are accessible.\")\n",
    "    traceback.print_exc()\n",
    "    raise e\n",
    "\n",
    "# Clear memory just in case\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "1830447c-4997-4507-abbf-35e2b2fa156b",
    "_uuid": "fb3501f0-afd7-400c-bdb7-55ac42d2d314",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-19T03:37:01.585840Z",
     "iopub.status.busy": "2025-04-19T03:37:01.585596Z",
     "iopub.status.idle": "2025-04-19T03:37:08.533974Z",
     "shell.execute_reply": "2025-04-19T03:37:08.533387Z",
     "shell.execute_reply.started": "2025-04-19T03:37:01.585822Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading unlabeled data from: /kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl\n",
      "Type of loaded pickle object: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Pickle contains HF Dataset. Using directly.\n",
      "Created unlabeled HF Dataset with 8000 examples.\n",
      "Columns: ['text']\n",
      "Tokenizing unlabeled data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25165fcbac44982b69c718eeefc953b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabeled data tokenized and formatted.\n"
     ]
    }
   ],
   "source": [
    "# --- Load and Preprocess Unlabeled Data ---\n",
    "print(f\"\\nLoading unlabeled data from: {unlabeled_data_path}\")\n",
    "try:\n",
    "    with open(unlabeled_data_path, \"rb\") as f:\n",
    "        test_unlabelled_pickle = pickle.load(f)\n",
    "\n",
    "    print(f\"Type of loaded pickle object: {type(test_unlabelled_pickle)}\")\n",
    "\n",
    "    # Convert pickle content to HF Dataset\n",
    "    id_source_df = None # For getting original IDs later if needed\n",
    "\n",
    "    if isinstance(test_unlabelled_pickle, Dataset):\n",
    "        print(\"Pickle contains HF Dataset. Using directly.\")\n",
    "        test_unlabelled_dataset_hf = test_unlabelled_pickle\n",
    "        id_col_present = 'id' in test_unlabelled_dataset_hf.column_names\n",
    "    elif isinstance(test_unlabelled_pickle, pd.DataFrame):\n",
    "        print(\"Pickle contains DataFrame. Converting...\")\n",
    "        id_source_df = test_unlabelled_pickle \n",
    "        if 'text' not in id_source_df.columns: raise KeyError(\"'text' column missing in DataFrame\")\n",
    "        test_unlabelled_dataset_hf = Dataset.from_pandas(id_source_df)\n",
    "        id_col_present = True \n",
    "    elif isinstance(test_unlabelled_pickle, dict):\n",
    "        print(\"Pickle contains dict. Converting...\")\n",
    "        if 'text' not in test_unlabelled_pickle: raise KeyError(\"'text' key missing in dict\")\n",
    "        test_unlabelled_dataset_hf = Dataset.from_dict(test_unlabelled_pickle)\n",
    "        id_col_present = 'id' in test_unlabelled_dataset_hf.column_names\n",
    "        # Add IDs if not present\n",
    "        if not id_col_present:\n",
    "            test_unlabelled_dataset_hf = test_unlabelled_dataset_hf.add_column(\"id\", range(len(test_unlabelled_dataset_hf)))\n",
    "            id_col_present = True \n",
    "    elif isinstance(test_unlabelled_pickle, list) and len(test_unlabelled_pickle) > 0 and isinstance(test_unlabelled_pickle[0], str):\n",
    "        print(\"Pickle contains list of strings. Creating dataset...\")\n",
    "        test_unlabelled_dataset_hf = Dataset.from_dict({\"text\": test_unlabelled_pickle})\n",
    "        test_unlabelled_dataset_hf = test_unlabelled_dataset_hf.add_column(\"id\", range(len(test_unlabelled_dataset_hf)))\n",
    "        id_col_present = True\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported data type loaded from pickle: {type(test_unlabelled_pickle)}\")\n",
    "\n",
    "    print(f\"Created unlabeled HF Dataset with {len(test_unlabelled_dataset_hf)} examples.\")\n",
    "    print(f\"Columns: {test_unlabelled_dataset_hf.column_names}\")\n",
    "    if 'text' not in test_unlabelled_dataset_hf.column_names:\n",
    "        raise KeyError(\"Resulting dataset must have a 'text' column for tokenization.\")\n",
    "\n",
    "    # --- Tokenize ---\n",
    "    print(\"Tokenizing unlabeled data...\")\n",
    "    def preprocess_unlabelled(examples):\n",
    "        # Tokenize and return PyTorch tensors, padding handled by DataLoader later if needed\n",
    "        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=TOKENIZER_MAX_LENGTH, return_tensors=\"pt\")\n",
    "\n",
    "    tokenized_unlabelled = test_unlabelled_dataset_hf.map(\n",
    "        preprocess_unlabelled,\n",
    "        batched=True,\n",
    "        remove_columns=['text'] + (['id'] if id_col_present else []) \n",
    "    )\n",
    "\n",
    "    # Set format to ensure __getitem__ returns tensors\n",
    "    tokenized_unlabelled.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    print(\"Unlabeled data tokenized and formatted.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Unlabeled test data not found at {unlabeled_data_path}.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading or preprocessing unlabeled data: {e}\")\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "0dbb560c-cb56-4d9a-8b48-497190b57bcc",
    "_uuid": "af684da8-8971-4f38-95f2-f036e45809be",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-19T03:37:08.534957Z",
     "iopub.status.busy": "2025-04-19T03:37:08.534685Z",
     "iopub.status.idle": "2025-04-19T03:39:10.289744Z",
     "shell.execute_reply": "2025-04-19T03:39:10.289047Z",
     "shell.execute_reply.started": "2025-04-19T03:37:08.534936Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction loop with batch size: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da56c8c254641338f2a3fc38e4b990b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished prediction loop. Total predictions: 8000\n"
     ]
    }
   ],
   "source": [
    "# --- Manual Prediction Loop ---\n",
    "print(f\"Starting prediction loop with batch size: {PREDICTION_BATCH_SIZE}\")\n",
    "all_preds = []\n",
    "# Use standard PyTorch DataLoader\n",
    "pred_dataloader = DataLoader(\n",
    "    tokenized_unlabelled,\n",
    "    batch_size=PREDICTION_BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Ensure no gradients are computed\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(pred_dataloader, desc=\"Predicting\"):\n",
    "        # Move batch to the same device as the model\n",
    "        try:\n",
    "            # Filter batch to only contain expected model inputs\n",
    "            model_inputs = {k: v.to(device) for k, v in batch.items() if k in ['input_ids', 'attention_mask']}\n",
    "            if not model_inputs:\n",
    "                 print(f\"Warning: Empty batch after filtering for model inputs? Batch keys: {batch.keys()}\")\n",
    "                 continue\n",
    "        except AttributeError:\n",
    "            print(f\"ERROR: Error moving batch to device. Batch keys: {batch.keys()}. Ensure data format is torch tensors.\")\n",
    "            raise\n",
    "\n",
    "        # Get model outputs\n",
    "        try:\n",
    "            outputs = model(**model_inputs)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            # Move predictions to CPU and convert to numpy list/array\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR during model prediction on batch: {e}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "print(f\"Finished prediction loop. Total predictions: {len(all_preds)}\")\n",
    "\n",
    "# Clear cache after prediction\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "8d46401a-8530-4320-86cb-c7e0635813c6",
    "_uuid": "2c089e8a-423d-445e-8c10-e310cc304d78",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-19T03:39:19.671647Z",
     "iopub.status.busy": "2025-04-19T03:39:19.670798Z",
     "iopub.status.idle": "2025-04-19T03:39:19.713480Z",
     "shell.execute_reply": "2025-04-19T03:39:19.712725Z",
     "shell.execute_reply.started": "2025-04-19T03:39:19.671608Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission DataFrame...\n",
      "Using generated sequential IDs for submission.\n",
      "Submission file saved to /kaggle/working/submission15.csv\n",
      "   ID  label\n",
      "0   0      3\n",
      "1   1      0\n",
      "2   2      0\n",
      "3   3      3\n",
      "4   4      2\n"
     ]
    }
   ],
   "source": [
    "# --- Create Submission File ---\n",
    "print(\"Creating submission DataFrame...\")\n",
    "try:\n",
    "    # Get IDs correctly\n",
    "    if 'id' in test_unlabelled_dataset_hf.column_names:\n",
    "         id_series = test_unlabelled_dataset_hf['id']\n",
    "         # If IDs were loaded as tensors, convert them\n",
    "         if isinstance(id_series, torch.Tensor): id_series = id_series.numpy()\n",
    "         elif not isinstance(id_series, (list, np.ndarray, pd.Series)): id_series = list(id_series) # Convert if it's some other iterable\n",
    "    elif id_source_df is not None: # If original was DataFrame\n",
    "         id_series = id_source_df.index\n",
    "    else: \n",
    "        id_series = range(len(all_preds))\n",
    "        print(\"Using generated sequential IDs for submission.\")\n",
    "\n",
    "    # Ensure lengths match\n",
    "    if len(id_series) != len(all_preds):\n",
    "        raise ValueError(f\"Mismatch between number of IDs ({len(id_series)}) and predictions ({len(all_preds)}). Check ID extraction logic.\")\n",
    "\n",
    "    submission_df = pd.DataFrame({\"ID\": id_series, \"label\": all_preds})\n",
    "\n",
    "    submission_file = \"/kaggle/working/submission15.csv\"\n",
    "    submission_df.to_csv(submission_file, index=False)\n",
    "    print(f\"Submission file saved to {submission_file}\")\n",
    "    print(submission_df.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during submission file creation: {e}\")\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11711500,
     "sourceId": 98084,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 303544,
     "modelInstanceId": 282680,
     "sourceId": 338014,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 303659,
     "modelInstanceId": 282796,
     "sourceId": 338185,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 303733,
     "modelInstanceId": 282870,
     "sourceId": 338280,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 303845,
     "modelInstanceId": 282983,
     "sourceId": 338430,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 303984,
     "modelInstanceId": 283120,
     "sourceId": 338592,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 307816,
     "modelInstanceId": 286997,
     "sourceId": 343142,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 307865,
     "modelInstanceId": 287047,
     "sourceId": 343252,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 308038,
     "modelInstanceId": 287222,
     "sourceId": 343479,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 309115,
     "modelInstanceId": 288339,
     "sourceId": 344935,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 309241,
     "modelInstanceId": 288476,
     "sourceId": 345189,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
