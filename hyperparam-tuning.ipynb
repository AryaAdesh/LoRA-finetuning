{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f851965e-2f05-48df-97fa-81722e82378d",
    "_uuid": "21cdccd0-3f9a-4e40-b6e9-0eef859fb1cd",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Notebook 3: Model Training with HPO (GPU)\n",
    "Purpose:\n",
    "1. Load pre-processed original and augmented AG News data.\n",
    "2. Define functions for HPO (model_init, hp_space).\n",
    "3. Use Trainer.hyperparameter_search with Optuna to find best params (tuning LR, decay, warmup, etc.).\n",
    "4. Train the final model using the best hyperparameters.\n",
    "5. Evaluate, save, visualize, and generate submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "524d8038-f28e-4675-929c-02d3772425fe",
    "_uuid": "9bf06a44-fd78-44d3-88af-71a5fbf74bd5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-15T04:41:07.230492Z",
     "iopub.status.busy": "2025-04-15T04:41:07.230193Z",
     "iopub.status.idle": "2025-04-15T04:41:10.356470Z",
     "shell.execute_reply": "2025-04-15T04:41:10.355646Z",
     "shell.execute_reply.started": "2025-04-15T04:41:07.230471Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up environment and installing Optuna...\n",
      "Optuna installed.\n",
      "INFO: Hugging Face datasets cache directory set to: /kaggle/working/hf_datasets_cache\n"
     ]
    }
   ],
   "source": [
    "# --- Essential Setup ---\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "import random\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "print(\"Setting up environment and installing Optuna...\")\n",
    "!rm -rf /kaggle/working/*\n",
    "!pip install -q optuna\n",
    "import optuna\n",
    "print(\"Optuna installed.\")\n",
    "\n",
    "# --- Cache Directory Setup ---\n",
    "cache_dir = \"/kaggle/working/hf_datasets_cache\"\n",
    "os.environ['HF_DATASETS_CACHE'] = cache_dir\n",
    "os.environ['DATASETS_CACHE'] = cache_dir\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "print(f\"INFO: Hugging Face datasets cache directory set to: {os.environ.get('HF_DATASETS_CACHE')}\")\n",
    "\n",
    "# --- Imports ---\n",
    "from datasets import load_dataset, Dataset, ClassLabel, load_from_disk, concatenate_datasets\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainerCallback,\n",
    "    SchedulerType\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, TaskType\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "bb259296-619f-4e63-96aa-0c5a994a7ec0",
    "_uuid": "592d9a51-9c1f-41c4-875b-20bb86194710",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-15T04:41:10.358315Z",
     "iopub.status.busy": "2025-04-15T04:41:10.358027Z",
     "iopub.status.idle": "2025-04-15T04:41:10.363689Z",
     "shell.execute_reply": "2025-04-15T04:41:10.362839Z",
     "shell.execute_reply.started": "2025-04-15T04:41:10.358291Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "base_model_name = 'roberta-base'\n",
    "dataset_name = 'ag_news'\n",
    "test_split_name = 'test'\n",
    "\n",
    "# Paths to pre-processed data from previous steps\n",
    "cleaned_original_load_path = \"/kaggle/input/cleanedorig\"\n",
    "tokenized_augmented_load_path = \"/kaggle/input/cleanedaugmenteddata\"\n",
    "\n",
    "# Output directories\n",
    "hpo_output_dir = \"/kaggle/working/results_hpo\" \n",
    "final_output_dir = \"/kaggle/working/results_final\"\n",
    "final_model_save_path = \"/kaggle/working/agnews_best_model_final\"\n",
    "\n",
    "LORA_R = 8 # IMPORTANT: Set your fixed rank based on param limit\n",
    "LORA_TARGET_MODULES = ['query', 'value'] # Or ['query', 'key', 'value'] if you prefer\n",
    "\n",
    "# Tokenizer settings\n",
    "TOKENIZER_MAX_LENGTH = 512\n",
    "\n",
    "# HPO settings\n",
    "N_HPO_TRIALS = 20 # Number of HPO trials to run (adjust based on time/resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "016027f8-8fc0-41f5-9254-fd817d02be67",
    "_uuid": "a8af2c1b-328f-4313-883a-2d905743d661",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-15T04:41:10.364972Z",
     "iopub.status.busy": "2025-04-15T04:41:10.364664Z",
     "iopub.status.idle": "2025-04-15T04:41:10.382047Z",
     "shell.execute_reply": "2025-04-15T04:41:10.381527Z",
     "shell.execute_reply.started": "2025-04-15T04:41:10.364956Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. Using device: cuda\n",
      "GPU Name: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "# --- GPU Check ---\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU is available. Using device: {device}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"WARNING: GPU not available, using CPU. HPO and Training will be very slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "cf8aa4ab-c255-4ea3-b0c4-e2595284ea51",
    "_uuid": "81231515-6844-4ac9-b365-a46afb60754c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-15T04:41:10.383445Z",
     "iopub.status.busy": "2025-04-15T04:41:10.383142Z",
     "iopub.status.idle": "2025-04-15T04:41:10.397219Z",
     "shell.execute_reply": "2025-04-15T04:41:10.396540Z",
     "shell.execute_reply.started": "2025-04-15T04:41:10.383424Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Label Info (Define explicitly)\n",
    "num_labels = 4\n",
    "id2label = {0: 'World', 1: 'Sports', 2: 'Business', 3: 'Sci/Tech'}\n",
    "label2id = {'World': 0, 'Sports': 1, 'Business': 2, 'Sci/Tech': 3}\n",
    "class_names = list(id2label.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "84e9f3ca-44d2-414b-a7ca-cb4b3c88acdd",
    "_uuid": "20785240-c880-4544-98ff-511bd78277b4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-15T04:41:10.399272Z",
     "iopub.status.busy": "2025-04-15T04:41:10.399056Z",
     "iopub.status.idle": "2025-04-15T04:41:10.633680Z",
     "shell.execute_reply": "2025-04-15T04:41:10.632828Z",
     "shell.execute_reply.started": "2025-04-15T04:41:10.399257Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: roberta-base\n"
     ]
    }
   ],
   "source": [
    "# --- Load Tokenizer ---\n",
    "print(f\"Loading tokenizer: {base_model_name}\")\n",
    "try:\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(base_model_name)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load tokenizer: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "0737b752-9126-4efb-b79c-186501495a07",
    "_uuid": "713b1818-0750-4672-b799-db36311a583d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-15T04:41:10.634605Z",
     "iopub.status.busy": "2025-04-15T04:41:10.634415Z",
     "iopub.status.idle": "2025-04-15T04:41:10.683171Z",
     "shell.execute_reply": "2025-04-15T04:41:10.682578Z",
     "shell.execute_reply.started": "2025-04-15T04:41:10.634589Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Attempting to load CLEANED ORIGINAL data from: /kaggle/input/cleanedorig\n",
      "INFO: Successfully loaded cleaned original data (114832 examples).\n",
      "INFO: Features: {'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': ClassLabel(names=['World', 'Sports', 'Business', 'Sci/Tech'], id=None)}\n",
      "INFO: Extracted ClassLabel features: ClassLabel(names=['World', 'Sports', 'Business', 'Sci/Tech'], id=None)\n",
      "INFO: Attempting to load TOKENIZED AUGMENTED data from: /kaggle/input/cleanedaugmenteddata\n",
      "INFO: Successfully loaded tokenized augmented data (114832 examples).\n",
      "INFO: Features: {'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': ClassLabel(names=['World', 'Sports', 'Business', 'Sci/Tech'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "# Load CLEANED ORIGINAL data\n",
    "print(f\"INFO: Attempting to load CLEANED ORIGINAL data from: {cleaned_original_load_path}\")\n",
    "if os.path.exists(cleaned_original_load_path):\n",
    "    try:\n",
    "        loaded_cleaned_original_ds = load_from_disk(cleaned_original_load_path)\n",
    "        print(f\"INFO: Successfully loaded cleaned original data ({len(loaded_cleaned_original_ds)} examples).\")\n",
    "        print(f\"INFO: Features: {loaded_cleaned_original_ds.features}\")\n",
    "        # Get the label feature info from the loaded dataset\n",
    "        if 'labels' in loaded_cleaned_original_ds.features and isinstance(loaded_cleaned_original_ds.features['labels'], ClassLabel):\n",
    "             original_labels_feature = loaded_cleaned_original_ds.features['labels']\n",
    "             print(f\"INFO: Extracted ClassLabel features: {original_labels_feature}\")\n",
    "        else:\n",
    "             raise ValueError(\"Loaded cleaned original dataset missing valid ClassLabel 'labels' feature.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading cleaned original dataset from disk: {e}.\")\n",
    "        raise RuntimeError(\"Failed to load cleaned original dataset\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Cleaned original dataset not found at {cleaned_original_load_path}\")\n",
    "\n",
    "# Load TOKENIZED AUGMENTED (from cleaned) data\n",
    "print(f\"INFO: Attempting to load TOKENIZED AUGMENTED data from: {tokenized_augmented_load_path}\")\n",
    "if os.path.exists(tokenized_augmented_load_path):\n",
    "    try:\n",
    "        loaded_tokenized_augmented_ds = load_from_disk(tokenized_augmented_load_path)\n",
    "        print(f\"INFO: Successfully loaded tokenized augmented data ({len(loaded_tokenized_augmented_ds)} examples).\")\n",
    "        print(f\"INFO: Features: {loaded_tokenized_augmented_ds.features}\")\n",
    "        # Verify/cast labels just in case save/load altered type\n",
    "        if loaded_tokenized_augmented_ds.features['labels'] != original_labels_feature:\n",
    "             print(\"WARNING: Loaded augmented dataset labels feature mismatch! Casting...\")\n",
    "             loaded_tokenized_augmented_ds = loaded_tokenized_augmented_ds.cast_column('labels', original_labels_feature)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading tokenized augmented dataset from disk: {e}.\")\n",
    "        raise RuntimeError(\"Failed to load tokenized augmented dataset\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Tokenized augmented dataset not found at {tokenized_augmented_load_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "1fa23432-05e4-4088-8f2f-adaa603e095a",
    "_uuid": "9dcffc8c-8997-4f40-a6f2-b6fc026f3408",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-15T04:41:10.684243Z",
     "iopub.status.busy": "2025-04-15T04:41:10.684005Z",
     "iopub.status.idle": "2025-04-15T04:41:15.189772Z",
     "shell.execute_reply": "2025-04-15T04:41:15.188887Z",
     "shell.execute_reply.started": "2025-04-15T04:41:10.684219Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Loading and preprocessing original TEST dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1f32cf3f98456aaa285d14442cb4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f550fa162d21424f86ad4529be2e4bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab6971b7a2d4daba902b70b0bb2d216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e442bd8094a451eb6503be21f50e9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Original test dataset processed (7600 examples).\n"
     ]
    }
   ],
   "source": [
    "print(\"INFO: Loading and preprocessing original TEST dataset...\")\n",
    "try:\n",
    "    original_test_dataset = load_dataset(dataset_name, split=test_split_name)\n",
    "    def preprocess_original_test(examples):\n",
    "        return tokenizer(examples['text'], truncation=True, padding=False, max_length=TOKENIZER_MAX_LENGTH)\n",
    "    num_cpus = os.cpu_count()\n",
    "    num_proc_initial = max(1, num_cpus - 2) if num_cpus > 2 else 1\n",
    "    tokenized_test_dataset = original_test_dataset.map(\n",
    "        preprocess_original_test, batched=True, num_proc=num_proc_initial, remove_columns=['text']\n",
    "    )\n",
    "    tokenized_test_dataset = tokenized_test_dataset.rename_column(\"label\", \"labels\")\n",
    "    # Cast test labels to match train labels feature\n",
    "    tokenized_test_dataset = tokenized_test_dataset.cast_column('labels', original_labels_feature)\n",
    "    eval_dataset = tokenized_test_dataset\n",
    "    print(f\"INFO: Original test dataset processed ({len(eval_dataset)} examples).\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load/process original test dataset: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "64c461b8-6127-4e6a-94e7-e4ada9118ce5",
    "_uuid": "78fe7ac3-115a-4a86-a610-e600b81657d8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-15T04:41:15.191610Z",
     "iopub.status.busy": "2025-04-15T04:41:15.191007Z",
     "iopub.status.idle": "2025-04-15T04:41:16.507615Z",
     "shell.execute_reply": "2025-04-15T04:41:16.507031Z",
     "shell.execute_reply.started": "2025-04-15T04:41:15.191584Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Combining CLEANED original and CLEANED augmented datasets...\n",
      "Combined dataset created (unshuffled) with 229664 examples.\n",
      "INFO: Saving unshuffled combined dataset temporarily to /kaggle/working/combined_dataset_temp_unshuffled...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a1b0cf1b1140c9977d25fbd5f986cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/229664 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Temporary save complete, memory cleared.\n",
      "INFO: Loading combined dataset from writable path: /kaggle/working/combined_dataset_temp_unshuffled\n",
      "INFO: Reloaded successfully.\n",
      "INFO: Shuffling the reloaded dataset...\n",
      "Combined training dataset ready with 229664 examples.\n",
      "Columns: ['input_ids', 'attention_mask', 'labels']\n",
      "INFO: Removing temporary save directory: /kaggle/working/combined_dataset_temp_unshuffled\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "# --- Combine Datasets ---\n",
    "print(\"INFO: Combining CLEANED original and CLEANED augmented datasets...\")\n",
    "required_columns = ['input_ids', 'attention_mask', 'labels']\n",
    "try:\n",
    "    train_dataset_for_concat = loaded_cleaned_original_ds.select_columns(required_columns)\n",
    "    tokenized_augmented_dataset_for_concat = loaded_tokenized_augmented_ds.select_columns(required_columns)\n",
    "except ValueError as e:\n",
    "    print(f\"ERROR: Column selection error: {e}. Check loaded dataset columns.\")\n",
    "    print(f\"Cleaned Original columns: {loaded_cleaned_original_ds.column_names}\")\n",
    "    print(f\"Cleaned Augmented columns: {loaded_tokenized_augmented_ds.column_names}\")\n",
    "    raise e\n",
    "\n",
    "combined_train_dataset_unshuffled = concatenate_datasets([train_dataset_for_concat, tokenized_augmented_dataset_for_concat])\n",
    "print(f\"Combined dataset created (unshuffled) with {len(combined_train_dataset_unshuffled)} examples.\")\n",
    "\n",
    "# --- Save temporarily to /kaggle/working ---\n",
    "# Define a temporary path in the writable directory\n",
    "temp_save_path = \"/kaggle/working/combined_dataset_temp_unshuffled\"\n",
    "print(f\"INFO: Saving unshuffled combined dataset temporarily to {temp_save_path}...\")\n",
    "try:\n",
    "    # Ensure the directory exists if saving nested datasets (though save_to_disk handles top level)\n",
    "    # os.makedirs(os.path.dirname(temp_save_path), exist_ok=True) # Usually not needed for top-level save\n",
    "    combined_train_dataset_unshuffled.save_to_disk(temp_save_path)\n",
    "    # Delete the in-memory object to free up RAM before loading it back\n",
    "    del combined_train_dataset_unshuffled\n",
    "    del train_dataset_for_concat # Also delete intermediate objects\n",
    "    del tokenized_augmented_dataset_for_concat\n",
    "    gc.collect() # Run garbage collection\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache() # Clear GPU cache if needed\n",
    "    print(\"INFO: Temporary save complete, memory cleared.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR saving temporary combined dataset: {e}\")\n",
    "    traceback.print_exc()\n",
    "    raise e\n",
    "\n",
    "# --- Load back from /kaggle/working ---\n",
    "# Now the dataset object's path is associated with a writable location\n",
    "print(f\"INFO: Loading combined dataset from writable path: {temp_save_path}\")\n",
    "try:\n",
    "    combined_train_dataset_reloaded = load_from_disk(temp_save_path)\n",
    "    print(\"INFO: Reloaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading temporary combined dataset: {e}\")\n",
    "    raise e\n",
    "\n",
    "# --- Shuffle the reloaded dataset ---\n",
    "print(\"INFO: Shuffling the reloaded dataset...\")\n",
    "try:\n",
    "    # Shuffle should now use /kaggle/working for any temporary files it needs\n",
    "    combined_train_dataset = combined_train_dataset_reloaded.shuffle(seed=42)\n",
    "    print(f\"Combined training dataset ready with {len(combined_train_dataset)} examples.\")\n",
    "    print(f\"Columns: {combined_train_dataset.column_names}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during shuffle after reload: {e}\")\n",
    "    traceback.print_exc()\n",
    "    raise e\n",
    "\n",
    "# --- Clean up ---\n",
    "del combined_train_dataset_reloaded # Delete the reloaded unshuffled version\n",
    "# Optional: remove the temporary directory to save space on /kaggle/working\n",
    "import shutil\n",
    "try:\n",
    "    print(f\"INFO: Removing temporary save directory: {temp_save_path}\")\n",
    "    shutil.rmtree(temp_save_path)\n",
    "except Exception as e:\n",
    "    print(f\"WARNING: Could not remove temporary directory {temp_save_path}: {e}\")\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "# --- Define eval_dataset ---\n",
    "eval_dataset = tokenized_test_dataset # Make sure this is defined before Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T04:41:16.508669Z",
     "iopub.status.busy": "2025-04-15T04:41:16.508421Z",
     "iopub.status.idle": "2025-04-15T04:41:16.519619Z",
     "shell.execute_reply": "2025-04-15T04:41:16.518884Z",
     "shell.execute_reply.started": "2025-04-15T04:41:16.508645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Adversarial Trainer Implementation ---\n",
    "from transformers import Trainer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import traceback\n",
    "\n",
    "# Define adversarial parameter (fixed for now, can be tuned via hp_space later)\n",
    "# Tune this epsilon value based on experiments (start small, e.g., 0.5, 1.0)\n",
    "ADVERSARIAL_EPSILON = 1.0\n",
    "\n",
    "class AdversarialTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer that incorporates FGM adversarial training.\n",
    "    \"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Override compute_loss to add Fast Gradient Method (FGM) adversarial training.\n",
    "        \"\"\"\n",
    "        # Store labels separately and remove from inputs if present\n",
    "        # to avoid issues when model calculates loss internally\n",
    "        labels = None\n",
    "        if \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\").clone() # Ensure labels are removed for **inputs\n",
    "\n",
    "        # --- Original Forward Pass ---\n",
    "        outputs = model(**inputs) # Pass inputs without labels initially\n",
    "        # Calculate original loss using model's output and stored labels\n",
    "        logits = outputs.logits\n",
    "        loss_fct = torch.nn.CrossEntropyLoss() # Assuming CrossEntropyLoss for classification\n",
    "        original_loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "        # Default to original loss if not training or if adversarial fails\n",
    "        loss_to_return = original_loss\n",
    "        adversarial_loss = None\n",
    "\n",
    "        # --- Adversarial Perturbation (Only during Training) ---\n",
    "        if self.is_in_train: # Use self.is_in_train provided by Trainer\n",
    "            try:\n",
    "                # 1. Get Embeddings\n",
    "                input_ids = inputs.get(\"input_ids\")\n",
    "                if input_ids is None:\n",
    "                    # If input_ids aren't passed, try getting them from the model maybe? Unlikely.\n",
    "                    # Or, if inputs_embeds were passed directly, use those.\n",
    "                    if inputs.get(\"inputs_embeds\") is not None:\n",
    "                         original_embeddings = inputs.get(\"inputs_embeds\")\n",
    "                    else:\n",
    "                         raise ValueError(\"Cannot perform adversarial training without input_ids or inputs_embeds\")\n",
    "                else:\n",
    "                     embedding_layer = model.get_input_embeddings()\n",
    "                     original_embeddings = embedding_layer(input_ids)\n",
    "\n",
    "                # 2. Calculate Gradients w.r.t. Embeddings\n",
    "                # Create a detached copy that requires gradients\n",
    "                embeds_for_grad = original_embeddings.detach().clone()\n",
    "                embeds_for_grad.requires_grad_(True)\n",
    "\n",
    "                # Prepare inputs for gradient calculation pass\n",
    "                grad_inputs = inputs.copy() # Use original inputs dict structure\n",
    "                grad_inputs[\"inputs_embeds\"] = embeds_for_grad\n",
    "                grad_inputs[\"input_ids\"] = None # Ensure input_ids is None if embeds are passed\n",
    "\n",
    "                # Forward pass ONLY to get gradients w.r.t. embeddings\n",
    "                # We use the already computed logits from the original pass if possible,\n",
    "                # or recompute if necessary. Recomputing is safer.\n",
    "                with torch.enable_grad(): # Ensure gradients are computed\n",
    "                    # Need model outputs corresponding to embeds_for_grad\n",
    "                    temp_outputs = model(**grad_inputs)\n",
    "                    temp_logits = temp_outputs.logits\n",
    "                    temp_loss = loss_fct(temp_logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "                # Calculate gradients w.r.t. embeds_for_grad\n",
    "                # Using torch.autograd.grad is generally cleaner if it works\n",
    "                embed_grads = torch.autograd.grad(temp_loss, embeds_for_grad, retain_graph=False, create_graph=False)[0]\n",
    "\n",
    "                if embed_grads is None:\n",
    "                    raise RuntimeError(\"Gradient w.r.t embeddings is None.\")\n",
    "\n",
    "                # 3. Calculate FGM Perturbation (L2 norm version)\n",
    "                norm = torch.norm(embed_grads, p=2, dim=-1, keepdim=True) + 1e-8\n",
    "                delta = embed_grads / norm\n",
    "                perturbation = ADVERSARIAL_EPSILON * delta\n",
    "\n",
    "                # 4. Apply Perturbation\n",
    "                perturbed_embeddings = original_embeddings.detach() + perturbation\n",
    "\n",
    "                # 5. Adversarial Forward Pass\n",
    "                adv_inputs = inputs.copy()\n",
    "                adv_inputs[\"inputs_embeds\"] = perturbed_embeddings\n",
    "                adv_inputs[\"input_ids\"] = None\n",
    "\n",
    "                adv_outputs = model(**adv_inputs)\n",
    "                adv_logits = adv_outputs.logits\n",
    "                adversarial_loss = loss_fct(adv_logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "                # 6. Combine Losses (Summing is common)\n",
    "                loss_to_return = original_loss + adversarial_loss\n",
    "                # Optional debug print:\n",
    "                # if self.state.global_step % 50 == 0: # Print periodically\n",
    "                #    print(f\"Step: {self.state.global_step}, Orig Loss: {original_loss.item():.4f}, Adv Loss: {adversarial_loss.item():.4f}, Total: {loss_to_return.item():.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"WARNING: Adversarial step failed at step {self.state.global_step if hasattr(self, 'state') else 'N/A'}: {e}. Using original loss.\")\n",
    "                # traceback.print_exc() # Uncomment for detailed debugging\n",
    "                loss_to_return = original_loss # Fall back to original loss\n",
    "\n",
    "        # Add labels back for Trainer's potential use (e.g., in evaluation)\n",
    "        if labels is not None:\n",
    "            inputs[\"labels\"] = labels\n",
    "\n",
    "        # Return loss, and potentially outputs if requested by the Trainer's internal logic\n",
    "        # The Trainer calls .backward() on the returned loss value\n",
    "        return (loss_to_return, outputs) if return_outputs else loss_to_return\n",
    "\n",
    "# --- End Adversarial Trainer Class ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_cell_guid": "99b870ac-b760-4f4d-b544-dc365c868c13",
    "_uuid": "fe42153c-7322-4ee6-9818-61ce5c3a5a60",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-15T04:41:16.520739Z",
     "iopub.status.busy": "2025-04-15T04:41:16.520483Z",
     "iopub.status.idle": "2025-04-15T04:41:16.541011Z",
     "shell.execute_reply": "2025-04-15T04:41:16.540419Z",
     "shell.execute_reply.started": "2025-04-15T04:41:16.520718Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Define Model Initialization Function (Simpler for HPO Search) ---\n",
    "# Using fixed LORA_R and initial LORA_ALPHA/LORA_DROPOUT during search\n",
    "INITIAL_LORA_ALPHA = LORA_R * 2 # Example: Start with alpha = 2*r during search\n",
    "INITIAL_LORA_DROPOUT = 0.1     # Example: Start with 0.1 dropout during search\n",
    "\n",
    "def model_init_for_hpo(trial=None): # trial argument often not needed here by default\n",
    "    print(f\"HPO Trial: Initializing base model + fixed r + initial alpha/dropout\")\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        base_model_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        r=LORA_R,                   # Your FIXED rank\n",
    "        lora_alpha=INITIAL_LORA_ALPHA, # Use initial fixed value for search\n",
    "        target_modules=LORA_TARGET_MODULES,\n",
    "        lora_dropout=INITIAL_LORA_DROPOUT # Use initial fixed value for search\n",
    "    )\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "    return peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "91d098c1-8bca-4873-b5f7-1dfebacd6504",
    "_uuid": "d6a0b9ad-6228-4ebb-9a9f-b570de7e6462",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-15T04:41:16.542076Z",
     "iopub.status.busy": "2025-04-15T04:41:16.541815Z",
     "iopub.status.idle": "2025-04-15T04:41:16.560681Z",
     "shell.execute_reply": "2025-04-15T04:41:16.560147Z",
     "shell.execute_reply.started": "2025-04-15T04:41:16.542053Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Define Hyperparameter Search Space for Optuna ---\n",
    "def hp_space(trial: optuna.trial.Trial) -> dict:\n",
    "    return {\n",
    "        # TrainingArguments parameters\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 8e-5, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [1]), # More epochs ok here\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.3),\n",
    "        \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 0.0, 0.2),\n",
    "        \"label_smoothing_factor\": trial.suggest_float(\"label_smoothing_factor\", 0.0, 0.2, step=0.05),\n",
    "        \"gradient_accumulation_steps\": trial.suggest_categorical(\"gradient_accumulation_steps\", [1, 2, 4]),\n",
    "\n",
    "        # Model configuration parameters (to be used AFTER search)\n",
    "        \"lora_alpha_multiplier\": trial.suggest_categorical(\"lora_alpha_multiplier\", [1, 2, 4]), # Suggest multiplier for fixed R\n",
    "        \"lora_dropout\": trial.suggest_float(\"lora_dropout\", 0.0, 0.2, step=0.05),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_cell_guid": "fea180cf-7d77-466e-b6d2-31a65a9c3995",
    "_uuid": "72502c34-be8b-4302-af23-80b4cb07fb51",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-15T04:41:16.561701Z",
     "iopub.status.busy": "2025-04-15T04:41:16.561465Z",
     "iopub.status.idle": "2025-04-15T04:41:16.599712Z",
     "shell.execute_reply": "2025-04-15T04:41:16.599194Z",
     "shell.execute_reply.started": "2025-04-15T04:41:16.561686Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining Initial Training Arguments for HPO...\n"
     ]
    }
   ],
   "source": [
    "# --- Define Initial Training Arguments (Some will be overridden by HPO) ---\n",
    "print(\"Defining Initial Training Arguments for HPO...\")\n",
    "# Note: output_dir here is temporary for HPO runs\n",
    "# num_train_epochs might be overridden by hp_space per trial\n",
    "training_args_for_hpo = TrainingArguments(\n",
    "    output_dir=hpo_output_dir,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,         # Evaluate relatively frequently during HPO\n",
    "    logging_steps=250,      # Log fairly often\n",
    "    save_steps=10000,       # Don't need to save checkpoints during HPO search itself\n",
    "    save_total_limit=1,     # Only keep one checkpoint (if save_steps is reached)\n",
    "\n",
    "    # These will likely be overridden by hp_space:\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,     # Low default for HPO trials\n",
    "    per_device_train_batch_size=16, # Keep this fixed based on memory\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=2, # Will be tuned by hp_space\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.06,\n",
    "    label_smoothing_factor=0.1,\n",
    "    # lora_alpha / lora_dropout are model config, set via model_init\n",
    "\n",
    "    # These control the HPO process evaluation:\n",
    "    load_best_model_at_end=False, # Don't load best during HPO search phase\n",
    "    metric_for_best_model=\"accuracy\", # Metric to optimize\n",
    "    greater_is_better=True,\n",
    "\n",
    "    fp16=torch.cuda.is_available(), # Use FP16 if GPU available\n",
    "\n",
    "    dataloader_num_workers=2,\n",
    "    report_to=[], # Disable default reporting like WandB during HPO\n",
    "    logging_dir=f\"{hpo_output_dir}/hpo_logs\", # Keep logs separate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_cell_guid": "c236be23-ead6-4aba-8556-6e175947ad5a",
    "_uuid": "2145bc7b-56c5-4210-9990-8a2632efbd09",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-15T04:41:16.600619Z",
     "iopub.status.busy": "2025-04-15T04:41:16.600392Z",
     "iopub.status.idle": "2025-04-15T04:41:16.608577Z",
     "shell.execute_reply": "2025-04-15T04:41:16.608036Z",
     "shell.execute_reply.started": "2025-04-15T04:41:16.600586Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Define Metrics, Collator, Callbacks ---\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "# Callbacks\n",
    "# MetricsCollectorCallback still uses logging internally to store logs,\n",
    "# but we don't rely on it for immediate cell output.\n",
    "class MetricsCollectorCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.logs = []\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            self.logs.append((state.global_step, logs))\n",
    "            # print(f\"Internal Log Collected: Step {state.global_step} - {logs}\") # Optional: noisy\n",
    "        return control\n",
    "\n",
    "# TrainingMetricsCallback modified to use print\n",
    "class TrainingMetricsCallback(TrainerCallback):\n",
    "     def on_train_batch_end(self, args, state, control, model=None, **kwargs):\n",
    "         if state.global_step % 50 == 0 and \"model_outputs\" in kwargs and \"inputs\" in kwargs: # Check periodically\n",
    "             try:\n",
    "                 outputs = kwargs[\"model_outputs\"]\n",
    "                 inputs = kwargs[\"inputs\"]\n",
    "                 if hasattr(outputs, \"logits\"):\n",
    "                     logits = outputs.logits.detach().cpu().numpy()\n",
    "                 elif isinstance(outputs, dict) and \"logits\" in outputs:\n",
    "                     logits = outputs[\"logits\"].detach().cpu().numpy()\n",
    "                 else: return control\n",
    "                 if \"labels\" not in inputs: return control\n",
    "\n",
    "                 labels = inputs[\"labels\"].detach().cpu().numpy()\n",
    "                 preds = np.argmax(logits, axis=-1)\n",
    "                 acc = accuracy_score(labels, preds)\n",
    "                 f1 = f1_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "                 # Use print instead of logging\n",
    "                 print(f\"Train Step {state.global_step} - Batch Metrics: Acc={acc:.4f}, F1={f1:.4f}\")\n",
    "             except Exception as e:\n",
    "                 print(f\"WARNING: Could not compute training batch metrics at step {state.global_step}: {e}\")\n",
    "         return control\n",
    "\n",
    "metrics_collector = MetricsCollectorCallback()\n",
    "training_metrics_callback = TrainingMetricsCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_cell_guid": "8e227b04-8a1f-49f2-8bd9-f5f265cbdff6",
    "_uuid": "3cb4e2a7-44dc-4c81-a2f7-6124ddbf225f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-15T04:41:16.610614Z",
     "iopub.status.busy": "2025-04-15T04:41:16.610425Z",
     "iopub.status.idle": "2025-04-15T04:41:17.017619Z",
     "shell.execute_reply": "2025-04-15T04:41:17.016711Z",
     "shell.execute_reply.started": "2025-04-15T04:41:16.610600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Trainer with model_init for HPO...\n",
      "HPO Trial: Initializing base model + fixed r + initial alpha/dropout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3344281309.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AdversarialTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_for_hpo = AdversarialTrainer(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized for HPO.\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize Trainer for HPO ---\n",
    "print(\"Initializing Trainer with model_init for HPO...\")\n",
    "# If using AdversarialTrainer, instantiate that class instead:\n",
    "# trainer_for_hpo = AdversarialTrainer(\n",
    "trainer_for_hpo = AdversarialTrainer(\n",
    "    model_init=model_init_for_hpo, # Use the model_init function\n",
    "    args=training_args_for_hpo,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=combined_train_dataset,\n",
    "    eval_dataset=eval_dataset,     # Use the AG News test set for evaluating HPO trials\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # No need for metrics_collector callback during HPO search\n",
    ")\n",
    "print(\"Trainer initialized for HPO.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c89da0f4-cff8-45b7-a2df-4990749ec48c",
    "_uuid": "a92b6e6d-c31c-457f-aa6b-3dd1c6046faa",
    "collapsed": false,
    "execution": {
     "execution_failed": "2025-04-15T16:11:27.634Z",
     "iopub.execute_input": "2025-04-15T04:53:52.359498Z",
     "iopub.status.busy": "2025-04-15T04:53:52.359156Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Optuna Pruner...\n",
      "Using Pruner: HyperbandPruner\n",
      "Initializing Trainer with model_init and Pruning Callback...\n",
      "OptunaPruningCallbackWithManualCheck Initialized.\n",
      "HPO Trial: Initializing base model + fixed r + initial alpha/dropout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3569083701.py:57: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AdversarialTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_for_hpo = AdversarialTrainer( # Or AdversarialTrainer(...)\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "[I 2025-04-15 04:53:52,828] A new study created in memory with name: no-name-8b7e273b-4876-4815-97d4-b588ecec4bed\n",
      "Trying to set lora_alpha_multiplier in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Trying to set lora_dropout in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized for HPO with Pruning Callback.\n",
      "Starting hyperparameter search with Optuna (20 trials)...\n",
      "HPO Trial: Initializing base model + fixed r + initial alpha/dropout\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3588' max='3588' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3588/3588 1:26:04, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.237400</td>\n",
       "      <td>1.658672</td>\n",
       "      <td>0.885658</td>\n",
       "      <td>0.885240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.179000</td>\n",
       "      <td>1.438830</td>\n",
       "      <td>0.895395</td>\n",
       "      <td>0.895257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.094500</td>\n",
       "      <td>1.380149</td>\n",
       "      <td>0.894474</td>\n",
       "      <td>0.894201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.078200</td>\n",
       "      <td>1.341694</td>\n",
       "      <td>0.896316</td>\n",
       "      <td>0.895955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.045700</td>\n",
       "      <td>1.305114</td>\n",
       "      <td>0.897500</td>\n",
       "      <td>0.897317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.028100</td>\n",
       "      <td>1.296962</td>\n",
       "      <td>0.898947</td>\n",
       "      <td>0.898658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.997500</td>\n",
       "      <td>1.298097</td>\n",
       "      <td>0.899211</td>\n",
       "      <td>0.898922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 06:20:00,138] Trial 0 finished with value: 0.8992105263157895 and parameters: {'learning_rate': 2.3993486539569273e-05, 'num_train_epochs': 1, 'weight_decay': 0.16964184665842208, 'warmup_ratio': 0.14611084220497558, 'label_smoothing_factor': 0.2, 'gradient_accumulation_steps': 4, 'lora_alpha_multiplier': 1, 'lora_dropout': 0.2}. Best is trial 0 with value: 0.8992105263157895.\n",
      "Trying to set lora_alpha_multiplier in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Trying to set lora_dropout in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO Trial: Initializing base model + fixed r + initial alpha/dropout\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14354' max='14354' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14354/14354 2:02:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.787600</td>\n",
       "      <td>2.784236</td>\n",
       "      <td>0.276316</td>\n",
       "      <td>0.149870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.605300</td>\n",
       "      <td>2.174863</td>\n",
       "      <td>0.855263</td>\n",
       "      <td>0.852934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.438700</td>\n",
       "      <td>1.589296</td>\n",
       "      <td>0.883026</td>\n",
       "      <td>0.882467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.237500</td>\n",
       "      <td>1.509378</td>\n",
       "      <td>0.892632</td>\n",
       "      <td>0.892414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.152400</td>\n",
       "      <td>1.449050</td>\n",
       "      <td>0.889737</td>\n",
       "      <td>0.889269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.119700</td>\n",
       "      <td>1.384844</td>\n",
       "      <td>0.894868</td>\n",
       "      <td>0.894635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.088700</td>\n",
       "      <td>1.355681</td>\n",
       "      <td>0.895789</td>\n",
       "      <td>0.895604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.041300</td>\n",
       "      <td>1.341303</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>0.894566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.088900</td>\n",
       "      <td>1.306831</td>\n",
       "      <td>0.898421</td>\n",
       "      <td>0.898123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.989500</td>\n",
       "      <td>1.310700</td>\n",
       "      <td>0.898289</td>\n",
       "      <td>0.898012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.982600</td>\n",
       "      <td>1.315387</td>\n",
       "      <td>0.899474</td>\n",
       "      <td>0.899320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.967800</td>\n",
       "      <td>1.282347</td>\n",
       "      <td>0.898947</td>\n",
       "      <td>0.898639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.904200</td>\n",
       "      <td>1.261578</td>\n",
       "      <td>0.900921</td>\n",
       "      <td>0.900757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.914200</td>\n",
       "      <td>1.223106</td>\n",
       "      <td>0.900132</td>\n",
       "      <td>0.899940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.913700</td>\n",
       "      <td>1.202627</td>\n",
       "      <td>0.902237</td>\n",
       "      <td>0.902066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.852900</td>\n",
       "      <td>1.157225</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.902251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.833100</td>\n",
       "      <td>1.171943</td>\n",
       "      <td>0.904605</td>\n",
       "      <td>0.904473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.753500</td>\n",
       "      <td>1.151852</td>\n",
       "      <td>0.904605</td>\n",
       "      <td>0.904381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.754400</td>\n",
       "      <td>1.097264</td>\n",
       "      <td>0.905395</td>\n",
       "      <td>0.905157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.771600</td>\n",
       "      <td>1.061280</td>\n",
       "      <td>0.905658</td>\n",
       "      <td>0.905481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.685600</td>\n",
       "      <td>1.102229</td>\n",
       "      <td>0.905921</td>\n",
       "      <td>0.905735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.698300</td>\n",
       "      <td>1.089790</td>\n",
       "      <td>0.905921</td>\n",
       "      <td>0.905603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.666900</td>\n",
       "      <td>1.135540</td>\n",
       "      <td>0.905658</td>\n",
       "      <td>0.905380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.655700</td>\n",
       "      <td>1.086975</td>\n",
       "      <td>0.906579</td>\n",
       "      <td>0.906345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.654300</td>\n",
       "      <td>1.074472</td>\n",
       "      <td>0.905921</td>\n",
       "      <td>0.905699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.680700</td>\n",
       "      <td>1.065355</td>\n",
       "      <td>0.906184</td>\n",
       "      <td>0.905972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.629200</td>\n",
       "      <td>1.105880</td>\n",
       "      <td>0.906184</td>\n",
       "      <td>0.905898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.622900</td>\n",
       "      <td>1.093674</td>\n",
       "      <td>0.905658</td>\n",
       "      <td>0.905373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 08:22:25,672] Trial 1 finished with value: 0.9056578947368421 and parameters: {'learning_rate': 2.7646451679018978e-05, 'num_train_epochs': 1, 'weight_decay': 0.06717801573840793, 'warmup_ratio': 0.18706404288326958, 'label_smoothing_factor': 0.0, 'gradient_accumulation_steps': 1, 'lora_alpha_multiplier': 1, 'lora_dropout': 0.1}. Best is trial 1 with value: 0.9056578947368421.\n",
      "Trying to set lora_alpha_multiplier in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Trying to set lora_dropout in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO Trial: Initializing base model + fixed r + initial alpha/dropout\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14354' max='14354' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14354/14354 2:02:21, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.757900</td>\n",
       "      <td>2.729067</td>\n",
       "      <td>0.574737</td>\n",
       "      <td>0.556490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.392000</td>\n",
       "      <td>1.582008</td>\n",
       "      <td>0.885526</td>\n",
       "      <td>0.885100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.265500</td>\n",
       "      <td>1.506019</td>\n",
       "      <td>0.891447</td>\n",
       "      <td>0.891064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.166600</td>\n",
       "      <td>1.476640</td>\n",
       "      <td>0.891974</td>\n",
       "      <td>0.891761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.125700</td>\n",
       "      <td>1.438041</td>\n",
       "      <td>0.893026</td>\n",
       "      <td>0.892635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.123100</td>\n",
       "      <td>1.399004</td>\n",
       "      <td>0.895526</td>\n",
       "      <td>0.895321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.098900</td>\n",
       "      <td>1.385061</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>0.894830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.059900</td>\n",
       "      <td>1.370346</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>0.894675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.119300</td>\n",
       "      <td>1.341063</td>\n",
       "      <td>0.895263</td>\n",
       "      <td>0.894962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.023100</td>\n",
       "      <td>1.354651</td>\n",
       "      <td>0.895395</td>\n",
       "      <td>0.895126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.030800</td>\n",
       "      <td>1.352802</td>\n",
       "      <td>0.897500</td>\n",
       "      <td>0.897254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.023200</td>\n",
       "      <td>1.335395</td>\n",
       "      <td>0.897895</td>\n",
       "      <td>0.897607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.974700</td>\n",
       "      <td>1.342383</td>\n",
       "      <td>0.899079</td>\n",
       "      <td>0.898847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.012600</td>\n",
       "      <td>1.316893</td>\n",
       "      <td>0.899079</td>\n",
       "      <td>0.898904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.022600</td>\n",
       "      <td>1.289008</td>\n",
       "      <td>0.898684</td>\n",
       "      <td>0.898500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.005200</td>\n",
       "      <td>1.287205</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.899687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.006500</td>\n",
       "      <td>1.280814</td>\n",
       "      <td>0.899605</td>\n",
       "      <td>0.899441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.947700</td>\n",
       "      <td>1.284398</td>\n",
       "      <td>0.899868</td>\n",
       "      <td>0.899578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.967000</td>\n",
       "      <td>1.269326</td>\n",
       "      <td>0.900658</td>\n",
       "      <td>0.900433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.003100</td>\n",
       "      <td>1.259042</td>\n",
       "      <td>0.900132</td>\n",
       "      <td>0.899927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.946900</td>\n",
       "      <td>1.267072</td>\n",
       "      <td>0.900526</td>\n",
       "      <td>0.900303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>1.260062</td>\n",
       "      <td>0.900395</td>\n",
       "      <td>0.900052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.958900</td>\n",
       "      <td>1.257342</td>\n",
       "      <td>0.900921</td>\n",
       "      <td>0.900716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>1.248538</td>\n",
       "      <td>0.901447</td>\n",
       "      <td>0.901238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.947100</td>\n",
       "      <td>1.248030</td>\n",
       "      <td>0.900789</td>\n",
       "      <td>0.900601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.965200</td>\n",
       "      <td>1.241700</td>\n",
       "      <td>0.900658</td>\n",
       "      <td>0.900456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.918900</td>\n",
       "      <td>1.249256</td>\n",
       "      <td>0.900921</td>\n",
       "      <td>0.900676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.924800</td>\n",
       "      <td>1.248710</td>\n",
       "      <td>0.901053</td>\n",
       "      <td>0.900797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 10:24:49,488] Trial 2 finished with value: 0.9010526315789473 and parameters: {'learning_rate': 1.7492051078303416e-05, 'num_train_epochs': 1, 'weight_decay': 0.29650518751275967, 'warmup_ratio': 0.041357842290825425, 'label_smoothing_factor': 0.15000000000000002, 'gradient_accumulation_steps': 1, 'lora_alpha_multiplier': 4, 'lora_dropout': 0.15000000000000002}. Best is trial 1 with value: 0.9056578947368421.\n",
      "Trying to set lora_alpha_multiplier in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Trying to set lora_dropout in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO Trial: Initializing base model + fixed r + initial alpha/dropout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7177' max='7177' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7177/7177 1:38:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.425400</td>\n",
       "      <td>1.499351</td>\n",
       "      <td>0.889737</td>\n",
       "      <td>0.889322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.093100</td>\n",
       "      <td>1.377672</td>\n",
       "      <td>0.895789</td>\n",
       "      <td>0.895560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.024600</td>\n",
       "      <td>1.278351</td>\n",
       "      <td>0.899605</td>\n",
       "      <td>0.899468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.935200</td>\n",
       "      <td>1.227840</td>\n",
       "      <td>0.902368</td>\n",
       "      <td>0.901877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.855900</td>\n",
       "      <td>1.167840</td>\n",
       "      <td>0.904474</td>\n",
       "      <td>0.904271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.770200</td>\n",
       "      <td>1.054429</td>\n",
       "      <td>0.905526</td>\n",
       "      <td>0.905242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.661300</td>\n",
       "      <td>1.022271</td>\n",
       "      <td>0.906842</td>\n",
       "      <td>0.906656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.622700</td>\n",
       "      <td>1.027880</td>\n",
       "      <td>0.907105</td>\n",
       "      <td>0.906811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>1.099203</td>\n",
       "      <td>0.904211</td>\n",
       "      <td>0.903785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.591800</td>\n",
       "      <td>0.976683</td>\n",
       "      <td>0.908684</td>\n",
       "      <td>0.908489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.570600</td>\n",
       "      <td>1.024600</td>\n",
       "      <td>0.907237</td>\n",
       "      <td>0.906956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.545500</td>\n",
       "      <td>1.038814</td>\n",
       "      <td>0.906579</td>\n",
       "      <td>0.906340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.543800</td>\n",
       "      <td>1.034056</td>\n",
       "      <td>0.905658</td>\n",
       "      <td>0.905453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.512200</td>\n",
       "      <td>1.034138</td>\n",
       "      <td>0.906842</td>\n",
       "      <td>0.906590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 12:02:57,732] Trial 3 finished with value: 0.9068421052631579 and parameters: {'learning_rate': 6.2426198959672e-05, 'num_train_epochs': 1, 'weight_decay': 0.018224213058098713, 'warmup_ratio': 0.06090936556667781, 'label_smoothing_factor': 0.05, 'gradient_accumulation_steps': 2, 'lora_alpha_multiplier': 1, 'lora_dropout': 0.1}. Best is trial 3 with value: 0.9068421052631579.\n",
      "Trying to set lora_alpha_multiplier in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Trying to set lora_dropout in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO Trial: Initializing base model + fixed r + initial alpha/dropout\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14354' max='14354' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14354/14354 2:01:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.715200</td>\n",
       "      <td>2.477549</td>\n",
       "      <td>0.809342</td>\n",
       "      <td>0.797798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.292800</td>\n",
       "      <td>1.523700</td>\n",
       "      <td>0.886316</td>\n",
       "      <td>0.885991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.204600</td>\n",
       "      <td>1.464739</td>\n",
       "      <td>0.892632</td>\n",
       "      <td>0.892228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.110600</td>\n",
       "      <td>1.425586</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.894533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.079600</td>\n",
       "      <td>1.377716</td>\n",
       "      <td>0.894605</td>\n",
       "      <td>0.894228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.066800</td>\n",
       "      <td>1.343527</td>\n",
       "      <td>0.896184</td>\n",
       "      <td>0.896004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.052900</td>\n",
       "      <td>1.334460</td>\n",
       "      <td>0.897368</td>\n",
       "      <td>0.897138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.012400</td>\n",
       "      <td>1.317190</td>\n",
       "      <td>0.898158</td>\n",
       "      <td>0.897774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.074400</td>\n",
       "      <td>1.296613</td>\n",
       "      <td>0.899868</td>\n",
       "      <td>0.899593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.972600</td>\n",
       "      <td>1.302864</td>\n",
       "      <td>0.898684</td>\n",
       "      <td>0.898422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.967300</td>\n",
       "      <td>1.317746</td>\n",
       "      <td>0.900526</td>\n",
       "      <td>0.900321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.962400</td>\n",
       "      <td>1.281835</td>\n",
       "      <td>0.900263</td>\n",
       "      <td>0.899958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.910700</td>\n",
       "      <td>1.283647</td>\n",
       "      <td>0.902105</td>\n",
       "      <td>0.901877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.927500</td>\n",
       "      <td>1.257189</td>\n",
       "      <td>0.901579</td>\n",
       "      <td>0.901392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.937700</td>\n",
       "      <td>1.226006</td>\n",
       "      <td>0.903553</td>\n",
       "      <td>0.903372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.905400</td>\n",
       "      <td>1.216327</td>\n",
       "      <td>0.903289</td>\n",
       "      <td>0.903014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.902600</td>\n",
       "      <td>1.210466</td>\n",
       "      <td>0.903553</td>\n",
       "      <td>0.903389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.841400</td>\n",
       "      <td>1.186827</td>\n",
       "      <td>0.904079</td>\n",
       "      <td>0.903822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.845300</td>\n",
       "      <td>1.163772</td>\n",
       "      <td>0.904342</td>\n",
       "      <td>0.904116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.859900</td>\n",
       "      <td>1.124741</td>\n",
       "      <td>0.903158</td>\n",
       "      <td>0.902978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.797900</td>\n",
       "      <td>1.159125</td>\n",
       "      <td>0.904868</td>\n",
       "      <td>0.904643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.778100</td>\n",
       "      <td>1.122391</td>\n",
       "      <td>0.904868</td>\n",
       "      <td>0.904561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.754300</td>\n",
       "      <td>1.137051</td>\n",
       "      <td>0.905526</td>\n",
       "      <td>0.905272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.739400</td>\n",
       "      <td>1.111527</td>\n",
       "      <td>0.906053</td>\n",
       "      <td>0.905863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.727000</td>\n",
       "      <td>1.095230</td>\n",
       "      <td>0.904737</td>\n",
       "      <td>0.904515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.737000</td>\n",
       "      <td>1.091054</td>\n",
       "      <td>0.905921</td>\n",
       "      <td>0.905680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.681400</td>\n",
       "      <td>1.109865</td>\n",
       "      <td>0.906316</td>\n",
       "      <td>0.906044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.675700</td>\n",
       "      <td>1.105577</td>\n",
       "      <td>0.906316</td>\n",
       "      <td>0.906033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 14:04:49,983] Trial 4 finished with value: 0.9063157894736842 and parameters: {'learning_rate': 2.5217218638599337e-05, 'num_train_epochs': 1, 'weight_decay': 0.034093832896985476, 'warmup_ratio': 0.04459190513152816, 'label_smoothing_factor': 0.0, 'gradient_accumulation_steps': 1, 'lora_alpha_multiplier': 2, 'lora_dropout': 0.05}. Best is trial 3 with value: 0.9068421052631579.\n",
      "Trying to set lora_alpha_multiplier in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Trying to set lora_dropout in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO Trial: Initializing base model + fixed r + initial alpha/dropout\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='7177' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/7177 20:27 < 1:17:30, 1.22 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.420900</td>\n",
       "      <td>1.872746</td>\n",
       "      <td>0.872105</td>\n",
       "      <td>0.871466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.376800</td>\n",
       "      <td>1.583091</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.889066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.276900</td>\n",
       "      <td>1.499515</td>\n",
       "      <td>0.891974</td>\n",
       "      <td>0.891711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 14:25:18,548] Trial 5 pruned. \n",
      "Trying to set lora_alpha_multiplier in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Trying to set lora_dropout in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO Trial: Initializing base model + fixed r + initial alpha/dropout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='14354' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1500/14354 12:46 < 1:49:33, 1.96 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.152800</td>\n",
       "      <td>1.755479</td>\n",
       "      <td>0.878289</td>\n",
       "      <td>0.877604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.302700</td>\n",
       "      <td>1.552196</td>\n",
       "      <td>0.886579</td>\n",
       "      <td>0.886261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.245300</td>\n",
       "      <td>1.500809</td>\n",
       "      <td>0.891447</td>\n",
       "      <td>0.891084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 14:38:06,272] Trial 6 pruned. \n",
      "Trying to set lora_alpha_multiplier in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Trying to set lora_dropout in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO Trial: Initializing base model + fixed r + initial alpha/dropout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6891' max='7177' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6891/7177 1:33:18 < 03:52, 1.23 it/s, Epoch 0.96/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.736100</td>\n",
       "      <td>2.630320</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.806305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.310200</td>\n",
       "      <td>1.519846</td>\n",
       "      <td>0.892368</td>\n",
       "      <td>0.892119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.153700</td>\n",
       "      <td>1.379949</td>\n",
       "      <td>0.896974</td>\n",
       "      <td>0.896826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.065400</td>\n",
       "      <td>1.343815</td>\n",
       "      <td>0.896447</td>\n",
       "      <td>0.896060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.015100</td>\n",
       "      <td>1.301609</td>\n",
       "      <td>0.897105</td>\n",
       "      <td>0.896853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>1.274174</td>\n",
       "      <td>0.899079</td>\n",
       "      <td>0.898789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.966200</td>\n",
       "      <td>1.253011</td>\n",
       "      <td>0.900132</td>\n",
       "      <td>0.899946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.944600</td>\n",
       "      <td>1.212001</td>\n",
       "      <td>0.902632</td>\n",
       "      <td>0.902321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.906500</td>\n",
       "      <td>1.221170</td>\n",
       "      <td>0.902763</td>\n",
       "      <td>0.902504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.906800</td>\n",
       "      <td>1.166971</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.902336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.892800</td>\n",
       "      <td>1.167820</td>\n",
       "      <td>0.904605</td>\n",
       "      <td>0.904330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.862700</td>\n",
       "      <td>1.156963</td>\n",
       "      <td>0.903684</td>\n",
       "      <td>0.903459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.854900</td>\n",
       "      <td>1.150437</td>\n",
       "      <td>0.904737</td>\n",
       "      <td>0.904531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, TrainerState, TrainerControl\n",
    "\n",
    "# --- 1. Create the Pruner Instance ---\n",
    "print(\"Creating Optuna Pruner...\")\n",
    "max_reports_per_epoch = (7500 // 500) # Rough estimate based on 1 epoch / 7500 steps\n",
    "# Choose your pruner (e.g., Hyperband or Median)\n",
    "pruner = optuna.pruners.HyperbandPruner(\n",
    "    min_resource=1,\n",
    "    max_resource=max_reports_per_epoch, \n",
    "    reduction_factor=3\n",
    ")\n",
    "# pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=6)\n",
    "print(f\"Using Pruner: {type(pruner).__name__}\")\n",
    "\n",
    "# --- 2. Define the Custom Pruning Callback ---\n",
    "class OptunaPruningCallbackWithManualCheck(TrainerCallback):\n",
    "    def __init__(self, pruner: optuna.pruners.BasePruner):\n",
    "        self.pruner = pruner\n",
    "        print(\"OptunaPruningCallbackWithManualCheck Initialized.\")\n",
    "\n",
    "    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, metrics: dict, **kwargs):\n",
    "        # The Trainer's HPO integration *should* make the optuna trial available via state\n",
    "        # We need to access it to report metrics and check for pruning\n",
    "        trial = getattr(state, \"trial\", None) # Safely get trial if it exists\n",
    "\n",
    "        if trial is None:\n",
    "            # This might happen if not running inside trainer.hyperparameter_search\n",
    "            # Or if the version doesn't expose it this way.\n",
    "            # print(\"Warning: Optuna trial not found in TrainerState during on_evaluate.\")\n",
    "            return control\n",
    "\n",
    "        # Get the metric value (must match compute_objective/metric_for_best_model)\n",
    "        metric_value = metrics.get(\"eval_accuracy\")\n",
    "        if metric_value is None:\n",
    "            print(f\"Warning: eval_accuracy not found in metrics for pruning check at step {state.global_step}.\")\n",
    "            return control\n",
    "\n",
    "        # Report the intermediate value to the trial\n",
    "        trial.report(metric_value, state.global_step)\n",
    "        print(f\"Trial {trial.number}: Reported metric {metric_value:.4f} at step {state.global_step}.\") # Debug print\n",
    "\n",
    "        # Ask the pruner (that we hold) if this trial should be pruned\n",
    "        if self.pruner.prune(study=trial.study, trial=trial): # Use study associated with trial\n",
    "            message = f\"Trial {trial.number} pruned at step {state.global_step}.\"\n",
    "            print(message)\n",
    "            raise optuna.TrialPruned(message)\n",
    "\n",
    "        return control\n",
    "\n",
    "# --- 3. Initialize Trainer with the Callback ---\n",
    "print(\"Initializing Trainer with model_init and Pruning Callback...\")\n",
    "\n",
    "# Create instance of the callback\n",
    "optuna_pruning_callback = OptunaPruningCallbackWithManualCheck(pruner)\n",
    "\n",
    "# Initialize Trainer (use AdversarialTrainer if applicable)\n",
    "trainer_for_hpo = AdversarialTrainer( # Or AdversarialTrainer(...)\n",
    "    model_init=model_init_for_hpo, # Your existing model_init\n",
    "    args=training_args_for_hpo,    # Your existing HPO args\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=combined_train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[optuna_pruning_callback], # ADD THE CUSTOM CALLBACK HERE\n",
    ")\n",
    "print(\"Trainer initialized for HPO with Pruning Callback.\")\n",
    "\n",
    "\n",
    "# --- 4. Run HPO Search (WITHOUT passing study) ---\n",
    "print(f\"Starting hyperparameter search with Optuna ({N_HPO_TRIALS} trials)...\")\n",
    "# Let the Trainer create the study internally, the callback will interact with it\n",
    "start_hpo_time = time.time()\n",
    "\n",
    "# Define objective computation (still needed for final value)\n",
    "def compute_objective(metrics: dict) -> float:\n",
    "    metric_value = metrics.get(\"eval_accuracy\")\n",
    "    if metric_value is None: return 0.0\n",
    "    return metric_value\n",
    "\n",
    "best_run = trainer_for_hpo.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=hp_space,\n",
    "    n_trials=N_HPO_TRIALS,\n",
    "    compute_objective=compute_objective,\n",
    "    # study=study, # DO NOT PASS STUDY HERE\n",
    ")\n",
    "\n",
    "end_hpo_time = time.time()\n",
    "print(\"\\n--- Hyperparameter Search Finished ---\")\n",
    "print(f\"HPO Duration: {end_hpo_time - start_hpo_time:.2f} seconds\")\n",
    "print(f\"Best Run Results:\")\n",
    "print(f\"  Trial ID (Optuna): {best_run.run_id}\") # run_id is Optuna's trial number\n",
    "print(f\"  Objective Value (eval_accuracy): {best_run.objective}\")\n",
    "print(f\"  Best Hyperparameters: {best_run.hyperparameters}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7146514,
     "sourceId": 11408837,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7146525,
     "sourceId": 11408863,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
